{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "n_residual_blocks = 5\n",
    "\n",
    "# The data, split between train and test sets\n",
    "def data_split(digit):\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    if digit != 'all':\n",
    "        if digit not in list(range(10)):\n",
    "            print(\"choose correct digit value [0-9] or 'all'\")\n",
    "            return\n",
    "        x_train=x_train[np.isin(y_train,[digit])]\n",
    "        x_test=x_test[np.isin(y_test,[digit])]\n",
    "\n",
    "    x_train = np.where(x_train < (0.33 * 256), 0, 1)\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    \n",
    "    x_test = np.where(x_test < (0.33 * 256), 0, 1)\n",
    "    x_test = x_test.astype(np.float32)        \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer is the PixelCNN layer. This layer simply\n",
    "# builds on the 2D convolutional layer, but includes masking.\n",
    "class PixelConvLayer(layers.Layer):\n",
    "    def __init__(self, mask_type, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mask_type = mask_type\n",
    "        self.conv = layers.Conv2D(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the conv2d layer to initialize kernel variables\n",
    "        self.conv.build(input_shape)\n",
    "        # Use the initialized kernel to create the mask\n",
    "        kernel_shape = self.conv.kernel.get_shape()\n",
    "        self.mask = np.zeros(shape=kernel_shape)\n",
    "        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n",
    "        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.conv.kernel.assign(self.conv.kernel * self.mask)\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "# Next, we build our residual block layer.\n",
    "# This is just a normal residual block, but based on the PixelConvLayer.\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "        self.pixel_conv = PixelConvLayer(\n",
    "            mask_type=\"B\",\n",
    "            filters=filters // 2,\n",
    "            kernel_size=3,\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2 = keras.layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pixel_conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return keras.layers.add([inputs, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5\n",
    "data_train, data_test = data_split(d)\n",
    "current_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " pixel_conv_layer (PixelConv  (None, 28, 28, 128)      6400      \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " residual_block (ResidualBlo  (None, 28, 28, 128)      98624     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " residual_block_1 (ResidualB  (None, 28, 28, 128)      98624     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_2 (ResidualB  (None, 28, 28, 128)      98624     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_3 (ResidualB  (None, 28, 28, 128)      98624     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_4 (ResidualB  (None, 28, 28, 128)      98624     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " pixel_conv_layer_6 (PixelCo  (None, 28, 28, 128)      16512     \n",
      " nvLayer)                                                        \n",
      "                                                                 \n",
      " pixel_conv_layer_7 (PixelCo  (None, 28, 28, 128)      16512     \n",
      " nvLayer)                                                        \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 28, 28, 1)         129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532,673\n",
      "Trainable params: 532,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=input_shape)\n",
    "x = PixelConvLayer(\n",
    "    mask_type=\"A\", filters=128, kernel_size=7, activation=\"relu\", padding=\"same\"\n",
    ")(inputs)\n",
    "\n",
    "for _ in range(n_residual_blocks):\n",
    "    x = ResidualBlock(filters=128)(x)\n",
    "\n",
    "for _ in range(2):\n",
    "    x = PixelConvLayer(\n",
    "        mask_type=\"B\",\n",
    "        filters=128,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        activation=\"relu\",\n",
    "        padding=\"valid\",\n",
    "    )(x)\n",
    "\n",
    "out = keras.layers.Conv2D(\n",
    "    filters=1, kernel_size=1, strides=1, activation=\"sigmoid\", padding=\"valid\"\n",
    ")(x)\n",
    "\n",
    "pixel_cnn = keras.Model(inputs, out)\n",
    "adam = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "pixel_cnn.compile(optimizer=adam, loss=\"binary_crossentropy\")\n",
    "\n",
    "pixel_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 - 19s - loss: 0.3234 - val_loss: 0.1292 - 19s/epoch - 444ms/step\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:58<00:00,  2.08s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "43/43 - 18s - loss: 0.1178 - val_loss: 0.1094 - 18s/epoch - 411ms/step\n",
      "Epoch 2/9\n",
      "43/43 - 18s - loss: 0.1075 - val_loss: 0.1050 - 18s/epoch - 411ms/step\n",
      "Epoch 3/9\n",
      "43/43 - 18s - loss: 0.1037 - val_loss: 0.1023 - 18s/epoch - 409ms/step\n",
      "Epoch 4/9\n",
      "43/43 - 18s - loss: 0.1016 - val_loss: 0.1009 - 18s/epoch - 411ms/step\n",
      "Epoch 5/9\n",
      "43/43 - 18s - loss: 0.1002 - val_loss: 0.0997 - 18s/epoch - 409ms/step\n",
      "Epoch 6/9\n",
      "43/43 - 17s - loss: 0.0983 - val_loss: 0.0985 - 17s/epoch - 406ms/step\n",
      "Epoch 7/9\n",
      "43/43 - 18s - loss: 0.0984 - val_loss: 0.0967 - 18s/epoch - 412ms/step\n",
      "Epoch 8/9\n",
      "43/43 - 18s - loss: 0.0965 - val_loss: 0.0958 - 18s/epoch - 413ms/step\n",
      "Epoch 9/9\n",
      "43/43 - 18s - loss: 0.0960 - val_loss: 0.0958 - 18s/epoch - 408ms/step\n"
     ]
    }
   ],
   "source": [
    "epoch = 9\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:58<00:00,  2.10s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAABwCAYAAACkaY2RAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAIQUlEQVR4nO3dQXLbOBAFUGkqV5j13P9Y2ecOmoUrFUW2aBIEwA/gvd1MOTSFBkCrqxu8Px6PGwAAAABZ/rn6BgAAAAD4TNIGAAAAIJCkDQAAAEAgSRsAAACAQJI2AAAAAIEkbQAAAAAC/Tjyw/f73fvBL/J4PO41riOGl/r1eDz+rXEhcbyOtTgFa3EC1uIUrMUJWItTsBYnYC1O4cu1qNIG+vl59Q0At9vNWoQU1iJksBYhw5drUdIGAAAAIJCkDQAAAEAgSRsAAACAQJI2AAAAAIEkbQAAAAACSdoAAAAABJK0AQAAAAj04+obAAAA4GuPx+Ov/77f7xfdCXAFlTYAAAAAgSRtAAAAAAJJ2gAAAAAEcqYNwIRe+9/36NkjX3J/R+j3v16NGIsjsIoje+ben7WHwhxU2gAAAAAEkrQBAAAACLRse5TS/Gult27AaGrsaa33ReZgnrRx1bh6tsJ1Wq/75+tb69fwnZMaVNoAAAAABJK0AQAAAAi0bHtUbUrTvne2PLB36biYkm7kNhXraywjzzW2bcXWOu1DDKC+2s+trbXoGTmmnnE7u5ertAEAAAAIJGkDAAAAEEjSBgAAACDQsmfaPPeVlfaz6TM+pmS89IjCWOyL40rZb1edQ6t+brY572ZuNb6P7L3+Clo+x678vpjyfB7BrGOl0gYAAAAgkKQNAAAAQKBl26OerVY6eJXncrW9Y77351qUwpXcL+M4MmfE3xjMqHQN1NhvzSc4z98pc3mN4d69drXYJ7a/tI7BajHeo2QejDyOKm0AAAAAAknaAAAAAASStAEAAAAI5EwbDtvbQ73Va1i7D7u0D5j5vMZ+5P7VrzhPhDPMH4AxrLzXjvB3/Mrxqal2rGeNi0obAAAAgECSNgAAAACBtEdxSmr5Yu3X09JO6/is9krA32b4DNRRew1osQJgRp5NbbT+W3+FuKm0AQAAAAgkaQMAAAAQSHsUh43WetS6zJ9jUsZ8tlLKI/N8ts++iq21U3tf7tm2aD4Co9u7P5NBTNrwJqh2VNoAAAAABJK0AQAAAAgkaQMAAAAQqPuZNl4VOpee58WUXs98uZ7Ylds7dkfG2Hki42gR/zQrnbl0ZZxmHldgfC33x9f9b+/vSjzHc7a9fLbP04pKGwAAAIBAkjYAAAAAgbq0R7Vsk1FSNb6er5Zln55jZg1vKy3p3StxfZgTfRwZ53fzpMY1Zrfq54bZWdvjKI2VGLfh77zjVNoAAAAABJK0AQAAAAjUpD2qZynZ1u9SesUZs8+fq0o+Zx9XzlvpbUK9lY6lGIxHzMbRug0WRpDwpqbW++aqa9vz6DyVNgAAAACBJG0AAAAAAknaAAAAAATq8srvvfb2u+3tB/Rq8PGJW12z9giv5N1YrtonzYfENWZOfi8xbnzNGYrQT+vzbXqu2Rr3n7rHeM73o9IGAAAAIJCkDQAAAECgS9ujarx6VLnq+MRpXGKXYysWylfpxVz7zD45jtrz9/V6teeCufVZyh4kNvWMNpYzt0OVSlmXr0YaZ5U2AAAAAIEkbQAAAAACSdoAAAAABIp65ffeV3Sn9sVBupJXKI7U78nXxJBW9O4zuhpzeO81/P06L/vY2qztMe3NPey9xl4lv0ulDQAAAEAgSRsAAACAQE3ao15Lft6VDW2VE5WWmSlPzFFjHrS4Dz4YF8iwyvNutPuFRNbRZz1bU4w/v2mJmkt6PFXaAAAAAASStAEAAAAI1OXtUSVvrCm5NtlazoMtNU4GB5idN0HBe+b2msSd3zwjP9t7FEaL31Xiyhaos/ev0gYAAAAgkKQNAAAAQCBJGwAAAIBAXc60eba3n+u152y2HsDVbcXTuUfA7Er2udK9LL3HHOCIvXvaKvuT8ciy8jiXfs9v9W96axl7lTYAAAAAgSRtAAAAAAJ1b4/aa+XSstWJPUC5FiXE9mVebc2z5PlSuj5G/byrEQtq0nrWRs9jMkokxlOlDQAAAEAgSRsAAACAQJI2AAAAAIFiz7QZjZ5HGIvzCb5nX8vSus+7dRyf79+cId2RMxferU3zvL/S55bnHawtfW2rtAEAAAAIJGkDAAAAEEh71AkJryTrRenvezO02bx+hlHum/d67k8zrIFV9IzHSs9I/pbcCre3LabG/PVsbaO0tensv3n9d+I5nxrz4pV5sq3GXvs8xrPGQqUNAAAAQCBJGwAAAIBA2qM4ZYTy95FL4a6yevnviCXtV97z1j6w+lxKkDIXxB9o4d3e0vpv1BGeb6n3lcQ86a92S9Se/z86lTYAAAAAgSRtAAAAAAJJ2gAAAAAEcqZNB7P21o2idi/pCOf41DTrGRU1Xhma5MpYbP3u0cd1RCln2MAIrjoPhfZK98KZYu88lSwjnpmYYvWxUmkDAAAAEEjSBgAAACCQ9iiW0roscdTSvdlahcgx6ppIcdX4zdoWORul9u0cGUvPzLk8x15saWmV9rXSdTTzmByl0gYAAAAgkKQNAAAAQCBJGwAAAIBAzrQ5YOW+1hqvxDx7jd6vblw53nut0osL/NF6z6ZcynNL7D8YhzGlrCP6qL1Oa3w3mk3p2Zm+Z/yh0gYAAAAgkKQNAAAAQKCj7VG/brfbzxY3MoILy7L+q3itqjGsMSatx7X0+g3uKzaOr1YsQdz5mYeJIZsuj2P6Gku/v1tADBMMEKfvTBHHCeJwxhQxfDZaPCvd73RxvIrvi98bbY119mUc7/o2AQAAAPJojwIAAAAIJGkDAAAAEEjSBgAAACCQpA0AAABAIEkbAAAAgECSNgAAAACBJG0AAAAAAknaAAAAAASStAEAAAAI9D871fsS7/gxCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "43/43 - 18s - loss: 0.0951 - val_loss: 0.0955 - 18s/epoch - 409ms/step\n",
      "Epoch 2/40\n",
      "43/43 - 18s - loss: 0.0945 - val_loss: 0.0935 - 18s/epoch - 409ms/step\n",
      "Epoch 3/40\n",
      "43/43 - 18s - loss: 0.0943 - val_loss: 0.0933 - 18s/epoch - 408ms/step\n",
      "Epoch 4/40\n",
      "43/43 - 18s - loss: 0.0936 - val_loss: 0.0952 - 18s/epoch - 410ms/step\n",
      "Epoch 5/40\n",
      "43/43 - 18s - loss: 0.0931 - val_loss: 0.0931 - 18s/epoch - 412ms/step\n",
      "Epoch 6/40\n",
      "43/43 - 18s - loss: 0.0935 - val_loss: 0.0924 - 18s/epoch - 409ms/step\n",
      "Epoch 7/40\n",
      "43/43 - 18s - loss: 0.0933 - val_loss: 0.0924 - 18s/epoch - 408ms/step\n",
      "Epoch 8/40\n",
      "43/43 - 18s - loss: 0.0924 - val_loss: 0.0920 - 18s/epoch - 412ms/step\n",
      "Epoch 9/40\n",
      "43/43 - 18s - loss: 0.0924 - val_loss: 0.0931 - 18s/epoch - 409ms/step\n",
      "Epoch 10/40\n",
      "43/43 - 18s - loss: 0.0919 - val_loss: 0.0912 - 18s/epoch - 408ms/step\n",
      "Epoch 11/40\n",
      "43/43 - 18s - loss: 0.0915 - val_loss: 0.0912 - 18s/epoch - 408ms/step\n",
      "Epoch 12/40\n",
      "43/43 - 18s - loss: 0.0914 - val_loss: 0.0912 - 18s/epoch - 407ms/step\n",
      "Epoch 13/40\n",
      "43/43 - 18s - loss: 0.0919 - val_loss: 0.0929 - 18s/epoch - 409ms/step\n",
      "Epoch 14/40\n",
      "43/43 - 18s - loss: 0.0913 - val_loss: 0.0908 - 18s/epoch - 412ms/step\n",
      "Epoch 15/40\n",
      "43/43 - 18s - loss: 0.0907 - val_loss: 0.0908 - 18s/epoch - 413ms/step\n",
      "Epoch 16/40\n",
      "43/43 - 18s - loss: 0.0905 - val_loss: 0.0914 - 18s/epoch - 410ms/step\n",
      "Epoch 17/40\n",
      "43/43 - 18s - loss: 0.0910 - val_loss: 0.0935 - 18s/epoch - 412ms/step\n",
      "Epoch 18/40\n",
      "43/43 - 18s - loss: 0.0906 - val_loss: 0.0902 - 18s/epoch - 409ms/step\n",
      "Epoch 19/40\n",
      "43/43 - 18s - loss: 0.0901 - val_loss: 0.0904 - 18s/epoch - 409ms/step\n",
      "Epoch 20/40\n",
      "43/43 - 18s - loss: 0.0901 - val_loss: 0.0910 - 18s/epoch - 408ms/step\n",
      "Epoch 21/40\n",
      "43/43 - 18s - loss: 0.0902 - val_loss: 0.0899 - 18s/epoch - 411ms/step\n",
      "Epoch 22/40\n",
      "43/43 - 18s - loss: 0.0896 - val_loss: 0.0903 - 18s/epoch - 410ms/step\n",
      "Epoch 23/40\n",
      "43/43 - 18s - loss: 0.0896 - val_loss: 0.0911 - 18s/epoch - 408ms/step\n",
      "Epoch 24/40\n",
      "43/43 - 18s - loss: 0.0896 - val_loss: 0.0911 - 18s/epoch - 411ms/step\n",
      "Epoch 25/40\n",
      "43/43 - 18s - loss: 0.0892 - val_loss: 0.0903 - 18s/epoch - 415ms/step\n",
      "Epoch 26/40\n",
      "43/43 - 18s - loss: 0.0890 - val_loss: 0.0910 - 18s/epoch - 411ms/step\n",
      "Epoch 27/40\n",
      "43/43 - 18s - loss: 0.0890 - val_loss: 0.0899 - 18s/epoch - 411ms/step\n",
      "Epoch 28/40\n",
      "43/43 - 18s - loss: 0.0887 - val_loss: 0.0894 - 18s/epoch - 412ms/step\n",
      "Epoch 29/40\n",
      "43/43 - 18s - loss: 0.0886 - val_loss: 0.0922 - 18s/epoch - 409ms/step\n",
      "Epoch 30/40\n",
      "43/43 - 18s - loss: 0.0889 - val_loss: 0.0899 - 18s/epoch - 410ms/step\n",
      "Epoch 31/40\n",
      "43/43 - 17s - loss: 0.0885 - val_loss: 0.0899 - 17s/epoch - 406ms/step\n",
      "Epoch 32/40\n",
      "43/43 - 18s - loss: 0.0883 - val_loss: 0.0908 - 18s/epoch - 413ms/step\n",
      "Epoch 33/40\n",
      "43/43 - 18s - loss: 0.0890 - val_loss: 0.0890 - 18s/epoch - 410ms/step\n",
      "Epoch 34/40\n",
      "43/43 - 18s - loss: 0.0881 - val_loss: 0.0891 - 18s/epoch - 409ms/step\n",
      "Epoch 35/40\n",
      "43/43 - 18s - loss: 0.0880 - val_loss: 0.0894 - 18s/epoch - 409ms/step\n",
      "Epoch 36/40\n",
      "43/43 - 18s - loss: 0.0878 - val_loss: 0.0892 - 18s/epoch - 410ms/step\n",
      "Epoch 37/40\n",
      "43/43 - 18s - loss: 0.0879 - val_loss: 0.0885 - 18s/epoch - 410ms/step\n",
      "Epoch 38/40\n",
      "43/43 - 18s - loss: 0.0880 - val_loss: 0.0889 - 18s/epoch - 416ms/step\n",
      "Epoch 39/40\n",
      "43/43 - 18s - loss: 0.0877 - val_loss: 0.0889 - 18s/epoch - 409ms/step\n",
      "Epoch 40/40\n",
      "43/43 - 18s - loss: 0.0875 - val_loss: 0.0884 - 18s/epoch - 412ms/step\n"
     ]
    }
   ],
   "source": [
    "epoch = 40\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:59<00:00,  2.11s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "43/43 - 18s - loss: 0.0873 - val_loss: 0.0896 - 18s/epoch - 409ms/step\n",
      "Epoch 2/50\n",
      "43/43 - 18s - loss: 0.0876 - val_loss: 0.0885 - 18s/epoch - 410ms/step\n",
      "Epoch 3/50\n",
      "43/43 - 18s - loss: 0.0871 - val_loss: 0.0886 - 18s/epoch - 411ms/step\n",
      "Epoch 4/50\n",
      "43/43 - 20s - loss: 0.0872 - val_loss: 0.0905 - 20s/epoch - 454ms/step\n",
      "Epoch 5/50\n",
      "43/43 - 22s - loss: 0.0874 - val_loss: 0.0891 - 22s/epoch - 510ms/step\n",
      "Epoch 6/50\n",
      "43/43 - 18s - loss: 0.0871 - val_loss: 0.0888 - 18s/epoch - 412ms/step\n",
      "Epoch 7/50\n",
      "43/43 - 18s - loss: 0.0873 - val_loss: 0.0897 - 18s/epoch - 414ms/step\n",
      "Epoch 8/50\n",
      "43/43 - 18s - loss: 0.0868 - val_loss: 0.0888 - 18s/epoch - 412ms/step\n",
      "Epoch 9/50\n",
      "43/43 - 18s - loss: 0.0870 - val_loss: 0.0888 - 18s/epoch - 411ms/step\n",
      "Epoch 10/50\n",
      "43/43 - 18s - loss: 0.0874 - val_loss: 0.0881 - 18s/epoch - 412ms/step\n",
      "Epoch 11/50\n",
      "43/43 - 20s - loss: 0.0866 - val_loss: 0.0879 - 20s/epoch - 455ms/step\n",
      "Epoch 12/50\n",
      "43/43 - 22s - loss: 0.0865 - val_loss: 0.0881 - 22s/epoch - 510ms/step\n",
      "Epoch 13/50\n",
      "43/43 - 18s - loss: 0.0866 - val_loss: 0.0883 - 18s/epoch - 408ms/step\n",
      "Epoch 14/50\n",
      "43/43 - 18s - loss: 0.0867 - val_loss: 0.0885 - 18s/epoch - 412ms/step\n",
      "Epoch 15/50\n",
      "43/43 - 18s - loss: 0.0869 - val_loss: 0.0890 - 18s/epoch - 412ms/step\n",
      "Epoch 16/50\n",
      "43/43 - 18s - loss: 0.0865 - val_loss: 0.0880 - 18s/epoch - 412ms/step\n",
      "Epoch 17/50\n",
      "43/43 - 18s - loss: 0.0861 - val_loss: 0.0882 - 18s/epoch - 409ms/step\n",
      "Epoch 18/50\n",
      "43/43 - 18s - loss: 0.0860 - val_loss: 0.0887 - 18s/epoch - 409ms/step\n",
      "Epoch 19/50\n",
      "43/43 - 18s - loss: 0.0861 - val_loss: 0.0889 - 18s/epoch - 412ms/step\n",
      "Epoch 20/50\n",
      "43/43 - 18s - loss: 0.0860 - val_loss: 0.0885 - 18s/epoch - 412ms/step\n",
      "Epoch 21/50\n",
      "43/43 - 18s - loss: 0.0860 - val_loss: 0.0887 - 18s/epoch - 409ms/step\n",
      "Epoch 22/50\n",
      "43/43 - 18s - loss: 0.0860 - val_loss: 0.0879 - 18s/epoch - 410ms/step\n",
      "Epoch 23/50\n",
      "43/43 - 18s - loss: 0.0860 - val_loss: 0.0879 - 18s/epoch - 409ms/step\n",
      "Epoch 24/50\n",
      "43/43 - 18s - loss: 0.0860 - val_loss: 0.0880 - 18s/epoch - 408ms/step\n",
      "Epoch 25/50\n",
      "43/43 - 18s - loss: 0.0857 - val_loss: 0.0885 - 18s/epoch - 410ms/step\n",
      "Epoch 26/50\n",
      "43/43 - 18s - loss: 0.0861 - val_loss: 0.0882 - 18s/epoch - 409ms/step\n",
      "Epoch 27/50\n",
      "43/43 - 18s - loss: 0.0857 - val_loss: 0.0881 - 18s/epoch - 411ms/step\n",
      "Epoch 28/50\n",
      "43/43 - 18s - loss: 0.0854 - val_loss: 0.0882 - 18s/epoch - 412ms/step\n",
      "Epoch 29/50\n",
      "43/43 - 18s - loss: 0.0857 - val_loss: 0.0879 - 18s/epoch - 410ms/step\n",
      "Epoch 30/50\n",
      "43/43 - 18s - loss: 0.0855 - val_loss: 0.0882 - 18s/epoch - 416ms/step\n",
      "Epoch 31/50\n",
      "43/43 - 18s - loss: 0.0857 - val_loss: 0.0894 - 18s/epoch - 412ms/step\n",
      "Epoch 32/50\n",
      "43/43 - 18s - loss: 0.0852 - val_loss: 0.0884 - 18s/epoch - 408ms/step\n",
      "Epoch 33/50\n",
      "43/43 - 18s - loss: 0.0853 - val_loss: 0.0887 - 18s/epoch - 409ms/step\n",
      "Epoch 34/50\n",
      "43/43 - 17s - loss: 0.0852 - val_loss: 0.0884 - 17s/epoch - 406ms/step\n",
      "Epoch 35/50\n",
      "43/43 - 18s - loss: 0.0854 - val_loss: 0.0885 - 18s/epoch - 408ms/step\n",
      "Epoch 36/50\n",
      "43/43 - 18s - loss: 0.0852 - val_loss: 0.0878 - 18s/epoch - 408ms/step\n",
      "Epoch 37/50\n",
      "43/43 - 18s - loss: 0.0851 - val_loss: 0.0895 - 18s/epoch - 410ms/step\n",
      "Epoch 38/50\n",
      "43/43 - 18s - loss: 0.0852 - val_loss: 0.0881 - 18s/epoch - 408ms/step\n",
      "Epoch 39/50\n",
      "43/43 - 17s - loss: 0.0849 - val_loss: 0.0883 - 17s/epoch - 406ms/step\n",
      "Epoch 40/50\n",
      "43/43 - 18s - loss: 0.0854 - val_loss: 0.0892 - 18s/epoch - 407ms/step\n",
      "Epoch 41/50\n",
      "43/43 - 18s - loss: 0.0849 - val_loss: 0.0884 - 18s/epoch - 411ms/step\n",
      "Epoch 42/50\n",
      "43/43 - 18s - loss: 0.0846 - val_loss: 0.0881 - 18s/epoch - 408ms/step\n",
      "Epoch 43/50\n",
      "43/43 - 18s - loss: 0.0847 - val_loss: 0.0878 - 18s/epoch - 407ms/step\n",
      "Epoch 44/50\n",
      "43/43 - 18s - loss: 0.0844 - val_loss: 0.0881 - 18s/epoch - 413ms/step\n",
      "Epoch 45/50\n",
      "43/43 - 18s - loss: 0.0845 - val_loss: 0.0880 - 18s/epoch - 412ms/step\n",
      "Epoch 46/50\n",
      "43/43 - 18s - loss: 0.0847 - val_loss: 0.0878 - 18s/epoch - 412ms/step\n",
      "Epoch 47/50\n",
      "43/43 - 18s - loss: 0.0844 - val_loss: 0.0885 - 18s/epoch - 413ms/step\n",
      "Epoch 48/50\n",
      "43/43 - 18s - loss: 0.0843 - val_loss: 0.0889 - 18s/epoch - 408ms/step\n",
      "Epoch 49/50\n",
      "43/43 - 18s - loss: 0.0843 - val_loss: 0.0880 - 18s/epoch - 409ms/step\n",
      "Epoch 50/50\n",
      "43/43 - 18s - loss: 0.0843 - val_loss: 0.0891 - 18s/epoch - 411ms/step\n"
     ]
    }
   ],
   "source": [
    "epoch = 50\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:58<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/43 - 18s - loss: 0.0845 - val_loss: 0.0880 - 18s/epoch - 412ms/step\n",
      "Epoch 2/100\n",
      "43/43 - 18s - loss: 0.0841 - val_loss: 0.0882 - 18s/epoch - 411ms/step\n",
      "Epoch 3/100\n",
      "43/43 - 18s - loss: 0.0839 - val_loss: 0.0878 - 18s/epoch - 409ms/step\n",
      "Epoch 4/100\n",
      "43/43 - 18s - loss: 0.0839 - val_loss: 0.0894 - 18s/epoch - 414ms/step\n",
      "Epoch 5/100\n",
      "43/43 - 18s - loss: 0.0839 - val_loss: 0.0883 - 18s/epoch - 413ms/step\n",
      "Epoch 6/100\n",
      "43/43 - 18s - loss: 0.0838 - val_loss: 0.0881 - 18s/epoch - 410ms/step\n",
      "Epoch 7/100\n",
      "43/43 - 18s - loss: 0.0837 - val_loss: 0.0883 - 18s/epoch - 409ms/step\n",
      "Epoch 8/100\n",
      "43/43 - 18s - loss: 0.0840 - val_loss: 0.0887 - 18s/epoch - 412ms/step\n",
      "Epoch 9/100\n",
      "43/43 - 18s - loss: 0.0836 - val_loss: 0.0879 - 18s/epoch - 414ms/step\n",
      "Epoch 10/100\n",
      "43/43 - 18s - loss: 0.0837 - val_loss: 0.0880 - 18s/epoch - 411ms/step\n",
      "Epoch 11/100\n",
      "43/43 - 18s - loss: 0.0836 - val_loss: 0.0880 - 18s/epoch - 409ms/step\n",
      "Epoch 12/100\n",
      "43/43 - 18s - loss: 0.0834 - val_loss: 0.0883 - 18s/epoch - 410ms/step\n",
      "Epoch 13/100\n",
      "43/43 - 18s - loss: 0.0835 - val_loss: 0.0882 - 18s/epoch - 412ms/step\n",
      "Epoch 14/100\n",
      "43/43 - 18s - loss: 0.0834 - val_loss: 0.0883 - 18s/epoch - 410ms/step\n",
      "Epoch 15/100\n",
      "43/43 - 18s - loss: 0.0834 - val_loss: 0.0883 - 18s/epoch - 410ms/step\n",
      "Epoch 16/100\n",
      "43/43 - 18s - loss: 0.0831 - val_loss: 0.0892 - 18s/epoch - 413ms/step\n",
      "Epoch 17/100\n",
      "43/43 - 18s - loss: 0.0831 - val_loss: 0.0884 - 18s/epoch - 410ms/step\n",
      "Epoch 18/100\n",
      "43/43 - 18s - loss: 0.0832 - val_loss: 0.0886 - 18s/epoch - 409ms/step\n",
      "Epoch 19/100\n",
      "43/43 - 18s - loss: 0.0830 - val_loss: 0.0884 - 18s/epoch - 411ms/step\n",
      "Epoch 20/100\n",
      "43/43 - 18s - loss: 0.0830 - val_loss: 0.0885 - 18s/epoch - 412ms/step\n",
      "Epoch 21/100\n",
      "43/43 - 18s - loss: 0.0829 - val_loss: 0.0882 - 18s/epoch - 408ms/step\n",
      "Epoch 22/100\n",
      "43/43 - 18s - loss: 0.0827 - val_loss: 0.0893 - 18s/epoch - 410ms/step\n",
      "Epoch 23/100\n",
      "43/43 - 18s - loss: 0.0827 - val_loss: 0.0885 - 18s/epoch - 410ms/step\n",
      "Epoch 24/100\n",
      "43/43 - 18s - loss: 0.0827 - val_loss: 0.0889 - 18s/epoch - 409ms/step\n",
      "Epoch 25/100\n",
      "43/43 - 18s - loss: 0.0827 - val_loss: 0.0891 - 18s/epoch - 410ms/step\n",
      "Epoch 26/100\n",
      "43/43 - 18s - loss: 0.0824 - val_loss: 0.0895 - 18s/epoch - 411ms/step\n",
      "Epoch 27/100\n",
      "43/43 - 18s - loss: 0.0827 - val_loss: 0.0896 - 18s/epoch - 408ms/step\n",
      "Epoch 28/100\n",
      "43/43 - 18s - loss: 0.0827 - val_loss: 0.0901 - 18s/epoch - 409ms/step\n",
      "Epoch 29/100\n",
      "43/43 - 18s - loss: 0.0825 - val_loss: 0.0888 - 18s/epoch - 410ms/step\n",
      "Epoch 30/100\n",
      "43/43 - 18s - loss: 0.0822 - val_loss: 0.0892 - 18s/epoch - 410ms/step\n",
      "Epoch 31/100\n",
      "43/43 - 18s - loss: 0.0823 - val_loss: 0.0889 - 18s/epoch - 411ms/step\n",
      "Epoch 32/100\n",
      "43/43 - 18s - loss: 0.0823 - val_loss: 0.0892 - 18s/epoch - 414ms/step\n",
      "Epoch 33/100\n",
      "43/43 - 18s - loss: 0.0820 - val_loss: 0.0895 - 18s/epoch - 410ms/step\n",
      "Epoch 34/100\n",
      "43/43 - 18s - loss: 0.0821 - val_loss: 0.0890 - 18s/epoch - 413ms/step\n",
      "Epoch 35/100\n",
      "43/43 - 18s - loss: 0.0821 - val_loss: 0.0893 - 18s/epoch - 411ms/step\n",
      "Epoch 36/100\n",
      "43/43 - 18s - loss: 0.0821 - val_loss: 0.0890 - 18s/epoch - 412ms/step\n",
      "Epoch 37/100\n",
      "43/43 - 18s - loss: 0.0818 - val_loss: 0.0898 - 18s/epoch - 412ms/step\n",
      "Epoch 38/100\n",
      "43/43 - 18s - loss: 0.0819 - val_loss: 0.0895 - 18s/epoch - 410ms/step\n",
      "Epoch 39/100\n",
      "43/43 - 18s - loss: 0.0815 - val_loss: 0.0898 - 18s/epoch - 410ms/step\n",
      "Epoch 40/100\n",
      "43/43 - 18s - loss: 0.0813 - val_loss: 0.0912 - 18s/epoch - 413ms/step\n",
      "Epoch 41/100\n",
      "43/43 - 18s - loss: 0.0816 - val_loss: 0.0899 - 18s/epoch - 408ms/step\n",
      "Epoch 42/100\n",
      "43/43 - 18s - loss: 0.0815 - val_loss: 0.0896 - 18s/epoch - 411ms/step\n",
      "Epoch 43/100\n",
      "43/43 - 18s - loss: 0.0815 - val_loss: 0.0894 - 18s/epoch - 412ms/step\n",
      "Epoch 44/100\n",
      "43/43 - 18s - loss: 0.0813 - val_loss: 0.0909 - 18s/epoch - 409ms/step\n",
      "Epoch 45/100\n",
      "43/43 - 18s - loss: 0.0811 - val_loss: 0.0894 - 18s/epoch - 409ms/step\n",
      "Epoch 46/100\n",
      "43/43 - 18s - loss: 0.0810 - val_loss: 0.0903 - 18s/epoch - 414ms/step\n",
      "Epoch 47/100\n",
      "43/43 - 18s - loss: 0.0811 - val_loss: 0.0895 - 18s/epoch - 411ms/step\n",
      "Epoch 48/100\n",
      "43/43 - 18s - loss: 0.0807 - val_loss: 0.0905 - 18s/epoch - 415ms/step\n",
      "Epoch 49/100\n",
      "43/43 - 18s - loss: 0.0810 - val_loss: 0.0899 - 18s/epoch - 408ms/step\n",
      "Epoch 50/100\n",
      "43/43 - 18s - loss: 0.0808 - val_loss: 0.0899 - 18s/epoch - 413ms/step\n",
      "Epoch 51/100\n",
      "43/43 - 17s - loss: 0.0806 - val_loss: 0.0900 - 17s/epoch - 406ms/step\n",
      "Epoch 52/100\n",
      "43/43 - 18s - loss: 0.0806 - val_loss: 0.0903 - 18s/epoch - 408ms/step\n",
      "Epoch 53/100\n",
      "43/43 - 18s - loss: 0.0804 - val_loss: 0.0900 - 18s/epoch - 413ms/step\n",
      "Epoch 54/100\n",
      "43/43 - 18s - loss: 0.0803 - val_loss: 0.0901 - 18s/epoch - 409ms/step\n",
      "Epoch 55/100\n",
      "43/43 - 18s - loss: 0.0805 - val_loss: 0.0903 - 18s/epoch - 410ms/step\n",
      "Epoch 56/100\n",
      "43/43 - 18s - loss: 0.0806 - val_loss: 0.0909 - 18s/epoch - 409ms/step\n",
      "Epoch 57/100\n",
      "43/43 - 18s - loss: 0.0802 - val_loss: 0.0903 - 18s/epoch - 413ms/step\n",
      "Epoch 58/100\n",
      "43/43 - 18s - loss: 0.0804 - val_loss: 0.0913 - 18s/epoch - 410ms/step\n",
      "Epoch 59/100\n",
      "43/43 - 18s - loss: 0.0801 - val_loss: 0.0922 - 18s/epoch - 411ms/step\n",
      "Epoch 60/100\n",
      "43/43 - 18s - loss: 0.0799 - val_loss: 0.0915 - 18s/epoch - 409ms/step\n",
      "Epoch 61/100\n",
      "43/43 - 18s - loss: 0.0797 - val_loss: 0.0911 - 18s/epoch - 408ms/step\n",
      "Epoch 62/100\n",
      "43/43 - 17s - loss: 0.0797 - val_loss: 0.0915 - 17s/epoch - 404ms/step\n",
      "Epoch 63/100\n",
      "43/43 - 18s - loss: 0.0796 - val_loss: 0.0918 - 18s/epoch - 412ms/step\n",
      "Epoch 64/100\n",
      "43/43 - 17s - loss: 0.0794 - val_loss: 0.0911 - 17s/epoch - 407ms/step\n",
      "Epoch 65/100\n",
      "43/43 - 18s - loss: 0.0797 - val_loss: 0.0918 - 18s/epoch - 410ms/step\n",
      "Epoch 66/100\n",
      "43/43 - 18s - loss: 0.0792 - val_loss: 0.0918 - 18s/epoch - 410ms/step\n",
      "Epoch 67/100\n",
      "43/43 - 18s - loss: 0.0793 - val_loss: 0.0913 - 18s/epoch - 409ms/step\n",
      "Epoch 68/100\n",
      "43/43 - 18s - loss: 0.0791 - val_loss: 0.0912 - 18s/epoch - 411ms/step\n",
      "Epoch 69/100\n",
      "43/43 - 18s - loss: 0.0793 - val_loss: 0.0911 - 18s/epoch - 412ms/step\n",
      "Epoch 70/100\n",
      "43/43 - 18s - loss: 0.0790 - val_loss: 0.0925 - 18s/epoch - 408ms/step\n",
      "Epoch 71/100\n",
      "43/43 - 18s - loss: 0.0793 - val_loss: 0.0928 - 18s/epoch - 412ms/step\n",
      "Epoch 72/100\n",
      "43/43 - 18s - loss: 0.0790 - val_loss: 0.0923 - 18s/epoch - 409ms/step\n",
      "Epoch 73/100\n",
      "43/43 - 18s - loss: 0.0789 - val_loss: 0.0919 - 18s/epoch - 411ms/step\n",
      "Epoch 74/100\n",
      "43/43 - 18s - loss: 0.0787 - val_loss: 0.0931 - 18s/epoch - 411ms/step\n",
      "Epoch 75/100\n",
      "43/43 - 18s - loss: 0.0786 - val_loss: 0.0921 - 18s/epoch - 414ms/step\n",
      "Epoch 76/100\n",
      "43/43 - 18s - loss: 0.0785 - val_loss: 0.0931 - 18s/epoch - 409ms/step\n",
      "Epoch 77/100\n",
      "43/43 - 18s - loss: 0.0785 - val_loss: 0.0916 - 18s/epoch - 409ms/step\n",
      "Epoch 78/100\n",
      "43/43 - 18s - loss: 0.0787 - val_loss: 0.0918 - 18s/epoch - 413ms/step\n",
      "Epoch 79/100\n",
      "43/43 - 18s - loss: 0.0785 - val_loss: 0.0934 - 18s/epoch - 414ms/step\n",
      "Epoch 80/100\n",
      "43/43 - 18s - loss: 0.0781 - val_loss: 0.0934 - 18s/epoch - 410ms/step\n",
      "Epoch 81/100\n",
      "43/43 - 18s - loss: 0.0779 - val_loss: 0.0927 - 18s/epoch - 413ms/step\n",
      "Epoch 82/100\n",
      "43/43 - 18s - loss: 0.0778 - val_loss: 0.0933 - 18s/epoch - 411ms/step\n",
      "Epoch 83/100\n",
      "43/43 - 18s - loss: 0.0781 - val_loss: 0.0957 - 18s/epoch - 407ms/step\n",
      "Epoch 84/100\n",
      "43/43 - 18s - loss: 0.0779 - val_loss: 0.0943 - 18s/epoch - 412ms/step\n",
      "Epoch 85/100\n",
      "43/43 - 17s - loss: 0.0777 - val_loss: 0.0929 - 17s/epoch - 407ms/step\n",
      "Epoch 86/100\n",
      "43/43 - 18s - loss: 0.0777 - val_loss: 0.0945 - 18s/epoch - 412ms/step\n",
      "Epoch 87/100\n",
      "43/43 - 18s - loss: 0.0781 - val_loss: 0.0936 - 18s/epoch - 408ms/step\n",
      "Epoch 88/100\n",
      "43/43 - 18s - loss: 0.0773 - val_loss: 0.0945 - 18s/epoch - 409ms/step\n",
      "Epoch 89/100\n",
      "43/43 - 18s - loss: 0.0772 - val_loss: 0.0949 - 18s/epoch - 410ms/step\n",
      "Epoch 90/100\n",
      "43/43 - 18s - loss: 0.0772 - val_loss: 0.0947 - 18s/epoch - 411ms/step\n",
      "Epoch 91/100\n",
      "43/43 - 18s - loss: 0.0772 - val_loss: 0.0950 - 18s/epoch - 414ms/step\n",
      "Epoch 92/100\n",
      "43/43 - 18s - loss: 0.0770 - val_loss: 0.0943 - 18s/epoch - 408ms/step\n",
      "Epoch 93/100\n",
      "43/43 - 18s - loss: 0.0768 - val_loss: 0.0954 - 18s/epoch - 411ms/step\n",
      "Epoch 94/100\n",
      "43/43 - 18s - loss: 0.0771 - val_loss: 0.0942 - 18s/epoch - 410ms/step\n",
      "Epoch 95/100\n",
      "43/43 - 18s - loss: 0.0767 - val_loss: 0.0951 - 18s/epoch - 409ms/step\n",
      "Epoch 96/100\n",
      "43/43 - 18s - loss: 0.0768 - val_loss: 0.0947 - 18s/epoch - 408ms/step\n",
      "Epoch 97/100\n",
      "43/43 - 18s - loss: 0.0765 - val_loss: 0.0954 - 18s/epoch - 414ms/step\n",
      "Epoch 98/100\n",
      "43/43 - 18s - loss: 0.0763 - val_loss: 0.0943 - 18s/epoch - 411ms/step\n",
      "Epoch 99/100\n",
      "43/43 - 18s - loss: 0.0764 - val_loss: 0.0949 - 18s/epoch - 412ms/step\n",
      "Epoch 100/100\n",
      "43/43 - 18s - loss: 0.0764 - val_loss: 0.0957 - 18s/epoch - 413ms/step\n",
      "current_epoch:  200\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch\n",
    "print('current_epoch: ', current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:58<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
