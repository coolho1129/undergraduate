{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "n_residual_blocks = 5\n",
    "\n",
    "# The data, split between train and test sets\n",
    "def data_split(digit):\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    if digit != 'all':\n",
    "        if digit not in list(range(10)):\n",
    "            print(\"choose correct digit value [0-9] or 'all'\")\n",
    "            return\n",
    "        x_train=x_train[np.isin(y_train,[digit])]\n",
    "        x_test=x_test[np.isin(y_test,[digit])]\n",
    "\n",
    "    x_train = np.where(x_train < (0.33 * 256), 0, 1)\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    \n",
    "    x_test = np.where(x_test < (0.33 * 256), 0, 1)\n",
    "    x_test = x_test.astype(np.float32)        \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer is the PixelCNN layer. This layer simply\n",
    "# builds on the 2D convolutional layer, but includes masking.\n",
    "class PixelConvLayer(layers.Layer):\n",
    "    def __init__(self, mask_type, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mask_type = mask_type\n",
    "        self.conv = layers.Conv2D(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the conv2d layer to initialize kernel variables\n",
    "        self.conv.build(input_shape)\n",
    "        # Use the initialized kernel to create the mask\n",
    "        kernel_shape = self.conv.kernel.get_shape()\n",
    "        self.mask = np.zeros(shape=kernel_shape)\n",
    "        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n",
    "        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.conv.kernel.assign(self.conv.kernel * self.mask)\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "# Next, we build our residual block layer.\n",
    "# This is just a normal residual block, but based on the PixelConvLayer.\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "        self.pixel_conv = PixelConvLayer(\n",
    "            mask_type=\"B\",\n",
    "            filters=filters // 2,\n",
    "            kernel_size=3,\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2 = keras.layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pixel_conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return keras.layers.add([inputs, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1\n",
    "data_train, data_test = data_split(d)\n",
    "current_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " pixel_conv_layer (PixelConv  (None, 28, 28, 128)      6400      \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " residual_block (ResidualBlo  (None, 28, 28, 128)      98624     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " residual_block_1 (ResidualB  (None, 28, 28, 128)      98624     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_2 (ResidualB  (None, 28, 28, 128)      98624     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_3 (ResidualB  (None, 28, 28, 128)      98624     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_4 (ResidualB  (None, 28, 28, 128)      98624     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " pixel_conv_layer_6 (PixelCo  (None, 28, 28, 128)      16512     \n",
      " nvLayer)                                                        \n",
      "                                                                 \n",
      " pixel_conv_layer_7 (PixelCo  (None, 28, 28, 128)      16512     \n",
      " nvLayer)                                                        \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 28, 28, 1)         129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532,673\n",
      "Trainable params: 532,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=input_shape)\n",
    "x = PixelConvLayer(\n",
    "    mask_type=\"A\", filters=128, kernel_size=7, activation=\"relu\", padding=\"same\"\n",
    ")(inputs)\n",
    "\n",
    "for _ in range(n_residual_blocks):\n",
    "    x = ResidualBlock(filters=128)(x)\n",
    "\n",
    "for _ in range(2):\n",
    "    x = PixelConvLayer(\n",
    "        mask_type=\"B\",\n",
    "        filters=128,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        activation=\"relu\",\n",
    "        padding=\"valid\",\n",
    "    )(x)\n",
    "\n",
    "out = keras.layers.Conv2D(\n",
    "    filters=1, kernel_size=1, strides=1, activation=\"sigmoid\", padding=\"valid\"\n",
    ")(x)\n",
    "\n",
    "pixel_cnn = keras.Model(inputs, out)\n",
    "adam = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "pixel_cnn.compile(optimizer=adam, loss=\"binary_crossentropy\")\n",
    "\n",
    "pixel_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 - 45s - loss: 0.2009 - val_loss: 0.0571 - 45s/epoch - 857ms/step\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:21<00:00,  2.92s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "53/53 - 42s - loss: 0.0542 - val_loss: 0.0506 - 42s/epoch - 802ms/step\n",
      "Epoch 2/9\n",
      "53/53 - 43s - loss: 0.0507 - val_loss: 0.0494 - 43s/epoch - 804ms/step\n",
      "Epoch 3/9\n",
      "53/53 - 43s - loss: 0.0497 - val_loss: 0.0488 - 43s/epoch - 809ms/step\n",
      "Epoch 4/9\n",
      "53/53 - 42s - loss: 0.0490 - val_loss: 0.0484 - 42s/epoch - 797ms/step\n",
      "Epoch 5/9\n",
      "53/53 - 35s - loss: 0.0486 - val_loss: 0.0473 - 35s/epoch - 653ms/step\n",
      "Epoch 6/9\n",
      "53/53 - 32s - loss: 0.0480 - val_loss: 0.0469 - 32s/epoch - 598ms/step\n",
      "Epoch 7/9\n",
      "53/53 - 32s - loss: 0.0477 - val_loss: 0.0467 - 32s/epoch - 606ms/step\n",
      "Epoch 8/9\n",
      "53/53 - 43s - loss: 0.0473 - val_loss: 0.0480 - 43s/epoch - 812ms/step\n",
      "Epoch 9/9\n",
      "53/53 - 43s - loss: 0.0469 - val_loss: 0.0462 - 43s/epoch - 805ms/step\n"
     ]
    }
   ],
   "source": [
    "epoch = 9\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:21<00:00,  2.92s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "53/53 - 30s - loss: 0.0463 - val_loss: 0.0454 - 30s/epoch - 561ms/step\n",
      "Epoch 2/40\n",
      "53/53 - 30s - loss: 0.0457 - val_loss: 0.0448 - 30s/epoch - 563ms/step\n",
      "Epoch 3/40\n",
      "53/53 - 29s - loss: 0.0452 - val_loss: 0.0447 - 29s/epoch - 556ms/step\n",
      "Epoch 4/40\n",
      "53/53 - 29s - loss: 0.0451 - val_loss: 0.0452 - 29s/epoch - 540ms/step\n",
      "Epoch 5/40\n",
      "53/53 - 29s - loss: 0.0452 - val_loss: 0.0442 - 29s/epoch - 544ms/step\n",
      "Epoch 6/40\n",
      "53/53 - 30s - loss: 0.0447 - val_loss: 0.0446 - 30s/epoch - 566ms/step\n",
      "Epoch 7/40\n",
      "53/53 - 30s - loss: 0.0449 - val_loss: 0.0442 - 30s/epoch - 559ms/step\n",
      "Epoch 8/40\n",
      "53/53 - 30s - loss: 0.0445 - val_loss: 0.0442 - 30s/epoch - 558ms/step\n",
      "Epoch 9/40\n",
      "53/53 - 30s - loss: 0.0445 - val_loss: 0.0443 - 30s/epoch - 559ms/step\n",
      "Epoch 10/40\n",
      "53/53 - 30s - loss: 0.0444 - val_loss: 0.0438 - 30s/epoch - 558ms/step\n",
      "Epoch 11/40\n",
      "53/53 - 30s - loss: 0.0445 - val_loss: 0.0438 - 30s/epoch - 564ms/step\n",
      "Epoch 12/40\n",
      "53/53 - 30s - loss: 0.0442 - val_loss: 0.0441 - 30s/epoch - 562ms/step\n",
      "Epoch 13/40\n",
      "53/53 - 28s - loss: 0.0441 - val_loss: 0.0436 - 28s/epoch - 532ms/step\n",
      "Epoch 14/40\n",
      "53/53 - 29s - loss: 0.0442 - val_loss: 0.0435 - 29s/epoch - 554ms/step\n",
      "Epoch 15/40\n",
      "53/53 - 30s - loss: 0.0441 - val_loss: 0.0438 - 30s/epoch - 560ms/step\n",
      "Epoch 16/40\n",
      "53/53 - 30s - loss: 0.0440 - val_loss: 0.0435 - 30s/epoch - 564ms/step\n",
      "Epoch 17/40\n",
      "53/53 - 30s - loss: 0.0439 - val_loss: 0.0436 - 30s/epoch - 560ms/step\n",
      "Epoch 18/40\n",
      "53/53 - 30s - loss: 0.0438 - val_loss: 0.0434 - 30s/epoch - 561ms/step\n",
      "Epoch 19/40\n",
      "53/53 - 30s - loss: 0.0438 - val_loss: 0.0436 - 30s/epoch - 561ms/step\n",
      "Epoch 20/40\n",
      "53/53 - 30s - loss: 0.0437 - val_loss: 0.0434 - 30s/epoch - 559ms/step\n",
      "Epoch 21/40\n",
      "53/53 - 30s - loss: 0.0437 - val_loss: 0.0437 - 30s/epoch - 562ms/step\n",
      "Epoch 22/40\n",
      "53/53 - 28s - loss: 0.0437 - val_loss: 0.0434 - 28s/epoch - 519ms/step\n",
      "Epoch 23/40\n",
      "53/53 - 30s - loss: 0.0435 - val_loss: 0.0434 - 30s/epoch - 562ms/step\n",
      "Epoch 24/40\n",
      "53/53 - 30s - loss: 0.0437 - val_loss: 0.0433 - 30s/epoch - 565ms/step\n",
      "Epoch 25/40\n",
      "53/53 - 30s - loss: 0.0435 - val_loss: 0.0438 - 30s/epoch - 562ms/step\n",
      "Epoch 26/40\n",
      "53/53 - 30s - loss: 0.0436 - val_loss: 0.0434 - 30s/epoch - 562ms/step\n",
      "Epoch 27/40\n",
      "53/53 - 30s - loss: 0.0435 - val_loss: 0.0432 - 30s/epoch - 557ms/step\n",
      "Epoch 28/40\n",
      "53/53 - 30s - loss: 0.0434 - val_loss: 0.0431 - 30s/epoch - 560ms/step\n",
      "Epoch 29/40\n",
      "53/53 - 30s - loss: 0.0435 - val_loss: 0.0431 - 30s/epoch - 559ms/step\n",
      "Epoch 30/40\n",
      "53/53 - 30s - loss: 0.0434 - val_loss: 0.0432 - 30s/epoch - 559ms/step\n",
      "Epoch 31/40\n",
      "53/53 - 28s - loss: 0.0433 - val_loss: 0.0432 - 28s/epoch - 522ms/step\n",
      "Epoch 32/40\n",
      "53/53 - 30s - loss: 0.0433 - val_loss: 0.0433 - 30s/epoch - 563ms/step\n",
      "Epoch 33/40\n",
      "53/53 - 30s - loss: 0.0433 - val_loss: 0.0432 - 30s/epoch - 562ms/step\n",
      "Epoch 34/40\n",
      "53/53 - 30s - loss: 0.0433 - val_loss: 0.0432 - 30s/epoch - 562ms/step\n",
      "Epoch 35/40\n",
      "53/53 - 30s - loss: 0.0433 - val_loss: 0.0433 - 30s/epoch - 557ms/step\n",
      "Epoch 36/40\n",
      "53/53 - 30s - loss: 0.0432 - val_loss: 0.0432 - 30s/epoch - 560ms/step\n",
      "Epoch 37/40\n",
      "53/53 - 30s - loss: 0.0431 - val_loss: 0.0432 - 30s/epoch - 563ms/step\n",
      "Epoch 38/40\n",
      "53/53 - 30s - loss: 0.0431 - val_loss: 0.0433 - 30s/epoch - 564ms/step\n",
      "Epoch 39/40\n",
      "53/53 - 30s - loss: 0.0431 - val_loss: 0.0431 - 30s/epoch - 565ms/step\n",
      "Epoch 40/40\n",
      "53/53 - 28s - loss: 0.0430 - val_loss: 0.0431 - 28s/epoch - 523ms/step\n"
     ]
    }
   ],
   "source": [
    "epoch = 40\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:01<00:00,  2.20s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "53/53 - 30s - loss: 0.0431 - val_loss: 0.0432 - 30s/epoch - 559ms/step\n",
      "Epoch 2/50\n",
      "53/53 - 30s - loss: 0.0430 - val_loss: 0.0436 - 30s/epoch - 560ms/step\n",
      "Epoch 3/50\n",
      "53/53 - 30s - loss: 0.0432 - val_loss: 0.0434 - 30s/epoch - 561ms/step\n",
      "Epoch 4/50\n",
      "53/53 - 30s - loss: 0.0430 - val_loss: 0.0431 - 30s/epoch - 562ms/step\n",
      "Epoch 5/50\n",
      "53/53 - 30s - loss: 0.0430 - val_loss: 0.0430 - 30s/epoch - 567ms/step\n",
      "Epoch 6/50\n",
      "53/53 - 28s - loss: 0.0429 - val_loss: 0.0437 - 28s/epoch - 525ms/step\n",
      "Epoch 7/50\n",
      "53/53 - 30s - loss: 0.0431 - val_loss: 0.0431 - 30s/epoch - 563ms/step\n",
      "Epoch 8/50\n",
      "53/53 - 30s - loss: 0.0430 - val_loss: 0.0430 - 30s/epoch - 565ms/step\n",
      "Epoch 9/50\n",
      "53/53 - 30s - loss: 0.0429 - val_loss: 0.0430 - 30s/epoch - 562ms/step\n",
      "Epoch 10/50\n",
      "53/53 - 30s - loss: 0.0429 - val_loss: 0.0429 - 30s/epoch - 564ms/step\n",
      "Epoch 11/50\n",
      "53/53 - 30s - loss: 0.0428 - val_loss: 0.0431 - 30s/epoch - 561ms/step\n",
      "Epoch 12/50\n",
      "53/53 - 30s - loss: 0.0429 - val_loss: 0.0432 - 30s/epoch - 560ms/step\n",
      "Epoch 13/50\n",
      "53/53 - 30s - loss: 0.0429 - val_loss: 0.0430 - 30s/epoch - 562ms/step\n",
      "Epoch 14/50\n",
      "53/53 - 30s - loss: 0.0428 - val_loss: 0.0430 - 30s/epoch - 566ms/step\n",
      "Epoch 15/50\n",
      "53/53 - 28s - loss: 0.0428 - val_loss: 0.0430 - 28s/epoch - 521ms/step\n",
      "Epoch 16/50\n",
      "53/53 - 30s - loss: 0.0429 - val_loss: 0.0430 - 30s/epoch - 563ms/step\n",
      "Epoch 17/50\n",
      "53/53 - 30s - loss: 0.0427 - val_loss: 0.0431 - 30s/epoch - 563ms/step\n",
      "Epoch 18/50\n",
      "53/53 - 30s - loss: 0.0427 - val_loss: 0.0431 - 30s/epoch - 562ms/step\n",
      "Epoch 19/50\n",
      "53/53 - 30s - loss: 0.0428 - val_loss: 0.0431 - 30s/epoch - 561ms/step\n",
      "Epoch 20/50\n",
      "53/53 - 30s - loss: 0.0428 - val_loss: 0.0431 - 30s/epoch - 566ms/step\n",
      "Epoch 21/50\n",
      "53/53 - 30s - loss: 0.0427 - val_loss: 0.0431 - 30s/epoch - 561ms/step\n",
      "Epoch 22/50\n",
      "53/53 - 30s - loss: 0.0426 - val_loss: 0.0431 - 30s/epoch - 558ms/step\n",
      "Epoch 23/50\n",
      "53/53 - 29s - loss: 0.0427 - val_loss: 0.0431 - 29s/epoch - 550ms/step\n",
      "Epoch 24/50\n",
      "53/53 - 28s - loss: 0.0426 - val_loss: 0.0432 - 28s/epoch - 529ms/step\n",
      "Epoch 25/50\n",
      "53/53 - 30s - loss: 0.0426 - val_loss: 0.0429 - 30s/epoch - 561ms/step\n",
      "Epoch 26/50\n",
      "53/53 - 30s - loss: 0.0425 - val_loss: 0.0435 - 30s/epoch - 561ms/step\n",
      "Epoch 27/50\n",
      "53/53 - 30s - loss: 0.0426 - val_loss: 0.0429 - 30s/epoch - 561ms/step\n",
      "Epoch 28/50\n",
      "53/53 - 30s - loss: 0.0425 - val_loss: 0.0436 - 30s/epoch - 564ms/step\n",
      "Epoch 29/50\n",
      "53/53 - 30s - loss: 0.0425 - val_loss: 0.0430 - 30s/epoch - 562ms/step\n",
      "Epoch 30/50\n",
      "53/53 - 30s - loss: 0.0425 - val_loss: 0.0432 - 30s/epoch - 562ms/step\n",
      "Epoch 31/50\n",
      "53/53 - 30s - loss: 0.0425 - val_loss: 0.0431 - 30s/epoch - 566ms/step\n",
      "Epoch 32/50\n",
      "53/53 - 29s - loss: 0.0425 - val_loss: 0.0430 - 29s/epoch - 541ms/step\n",
      "Epoch 33/50\n",
      "53/53 - 23s - loss: 0.0425 - val_loss: 0.0432 - 23s/epoch - 435ms/step\n",
      "Epoch 34/50\n",
      "53/53 - 22s - loss: 0.0424 - val_loss: 0.0430 - 22s/epoch - 414ms/step\n",
      "Epoch 35/50\n",
      "53/53 - 22s - loss: 0.0424 - val_loss: 0.0431 - 22s/epoch - 411ms/step\n",
      "Epoch 36/50\n",
      "53/53 - 22s - loss: 0.0424 - val_loss: 0.0430 - 22s/epoch - 409ms/step\n",
      "Epoch 37/50\n",
      "53/53 - 22s - loss: 0.0423 - val_loss: 0.0435 - 22s/epoch - 411ms/step\n",
      "Epoch 38/50\n",
      "53/53 - 23s - loss: 0.0423 - val_loss: 0.0431 - 23s/epoch - 439ms/step\n",
      "Epoch 39/50\n",
      "53/53 - 30s - loss: 0.0423 - val_loss: 0.0430 - 30s/epoch - 561ms/step\n",
      "Epoch 40/50\n",
      "53/53 - 30s - loss: 0.0423 - val_loss: 0.0431 - 30s/epoch - 561ms/step\n",
      "Epoch 41/50\n",
      "53/53 - 30s - loss: 0.0423 - val_loss: 0.0432 - 30s/epoch - 559ms/step\n",
      "Epoch 42/50\n",
      "53/53 - 30s - loss: 0.0423 - val_loss: 0.0437 - 30s/epoch - 560ms/step\n",
      "Epoch 43/50\n",
      "53/53 - 30s - loss: 0.0423 - val_loss: 0.0432 - 30s/epoch - 563ms/step\n",
      "Epoch 44/50\n",
      "53/53 - 30s - loss: 0.0422 - val_loss: 0.0432 - 30s/epoch - 560ms/step\n",
      "Epoch 45/50\n",
      "53/53 - 30s - loss: 0.0423 - val_loss: 0.0432 - 30s/epoch - 564ms/step\n",
      "Epoch 46/50\n",
      "53/53 - 30s - loss: 0.0422 - val_loss: 0.0435 - 30s/epoch - 564ms/step\n",
      "Epoch 47/50\n",
      "53/53 - 28s - loss: 0.0422 - val_loss: 0.0431 - 28s/epoch - 525ms/step\n",
      "Epoch 48/50\n",
      "53/53 - 30s - loss: 0.0421 - val_loss: 0.0432 - 30s/epoch - 558ms/step\n",
      "Epoch 49/50\n",
      "53/53 - 30s - loss: 0.0421 - val_loss: 0.0432 - 30s/epoch - 559ms/step\n",
      "Epoch 50/50\n",
      "53/53 - 30s - loss: 0.0420 - val_loss: 0.0432 - 30s/epoch - 562ms/step\n"
     ]
    }
   ],
   "source": [
    "epoch = 50\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:01<00:00,  2.21s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAABwCAYAAACkaY2RAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFQUlEQVR4nO3dQXLbMAwFUKmTK3Td+x8r+9xBXVbj2o6l0OIn+N42HY9TGLLyB6DWbdsWAAAAALL86v0GAAAAAPif0AYAAAAgkNAGAAAAIJDQBgAAACCQ0AYAAAAgkNAGAAAAINDHkX+8rqvng3eybdva4nXUsKuvbdt+t3ghdexHL5agFwvQiyXoxQL0Ygl6sQC9WMLdXjRpA9f57P0GgGVZ9CKk0IuQQS9Chru9KLQBAAAACCS0AQAAAAgktAEAAAAIJLQBAAAACCS0AQAAAAgktAEAAAAIJLQBAAAACCS0AQAAAAgktAEAAAAIJLQBAAAACCS0AQAAAAj00fsNwJW2bXv4s3VdL3wn3KM+QGWucXN5Vu89tecenx+qGO27L/H9mrQBAAAACCS0AQAAAAgktAEAAAAIJLQBAAAACCS0AQAAAAjk6VEAANDAq0/8gVb2n7nEJ/HACNKv3SZtAAAAAAIJbQAAAAACCW0AAAAAAjnThvLSdxRnpz5AZa5x9Z2psbNHaMVnCeozaQMAAAAQSGgDAAAAEMh6FAAAvMjKG8D4Xr2WJ6wgmrQBAAAACCS0AQAAAAgktAEAAAAIVO5Mm0e7aQm7aOTxubjekbMA1Ad+bqSd7Qqcd8IjegyAM0zaAAAAAAQS2gAAAAAEKrceBctiPB2AfNZlxnHmvkJ9AWjBpA0AAABAIKENAAAAQKDh16OswQAA8FMt7imtRAH4G701kzYAAAAAgYQ2AAAAAIGENgAAAACBhj/TBo6waw4AtOTeAiDfq+fsJF7TTdoAAAAABBLaAAAAAAQacj3KI8S45TMBAPxE4kg85x25N1R7IJlJGwAAAIBAQhsAAACAQEOuRwE1GU8GRmRFFzLoRaAikzYAAAAAgYQ2AAAAAIGENgAAAACBhjnTxo4qZzknBYAEvo+gvVf/RtB/kEEvHmfSBgAAACCQ0AYAAAAg0DDrUXDLyhwAAJDi7N8nVobeb+T/Y5M2AAAAAIGENgAAAACBhDYAAAAAgZxpQ0kj7ywCAPCa/T3fs/NE9j9zn0hLzrHh3UzaAAAAAAQS2gAAAAAEsh7FMDziGwCAGZxZ+3r2GsC4TNoAAAAABBLaAAAAAAQqvR5lJHAeag1wjOsmwBhur9eODOhPDbiSSRsAAACAQEIbAAAAgEBCGwAAAIBApc+02e8a2t0fk33ReuxlA9W4x4B8M/TpDL9jL+5X6cmkDQAAAEAgoQ0AAABAoGHWo/bjfsbTWBYjoFWoYy2vXp/VHYDWZvlumeX37M3fnKQwaQMAAAAQSGgDAAAAEEhoAwAAABBomDNtmIf9URiLngUARud+hlQmbQAAAAACCW0AAAAAAlmPojuPCIb53Pa9/m7n2TXV/zMA/HPlStRM38Ez/a5XMGkDAAAAEEhoAwAAABDIehQAlzM2CwAA3zNpAwAAABBIaAMAAAAQSGgDAAAAEMiZNnThMd8AAMC7eaw3ozNpAwAAABBIaAMAAAAQyHoUl7ESBfAez66vrqnwXo/6T+9BXfqbK5m0AQAAAAgktAEAAAAIJLQBAAAACORMG+A052jM6cpHZwIAHNX6XsV9LT2ZtAEAAAAIJLQBAAAACHR0PeprWZbPd7yRIyYcT/vT8LW61XDCut0qUce9CWtaroZnFKh7uToWqMlR5Wo4qRJ1nLD/9krUkHp1nLAvy9VwUnfruDqbAAAAACCP9SgAAACAQEIbAAAAgEBCGwAAAIBAQhsAAACAQEIbAAAAgEBCGwAAAIBAQhsAAACAQEIbAAAAgEBCGwAAAIBAfwHhnQ7PNW3r5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 - 30s - loss: 0.0422 - val_loss: 0.0432 - 30s/epoch - 566ms/step\n",
      "Epoch 2/100\n",
      "53/53 - 30s - loss: 0.0421 - val_loss: 0.0431 - 30s/epoch - 559ms/step\n",
      "Epoch 3/100\n",
      "53/53 - 28s - loss: 0.0421 - val_loss: 0.0433 - 28s/epoch - 525ms/step\n",
      "Epoch 4/100\n",
      "53/53 - 30s - loss: 0.0421 - val_loss: 0.0433 - 30s/epoch - 563ms/step\n",
      "Epoch 5/100\n",
      "53/53 - 30s - loss: 0.0419 - val_loss: 0.0434 - 30s/epoch - 560ms/step\n",
      "Epoch 6/100\n",
      "53/53 - 30s - loss: 0.0419 - val_loss: 0.0431 - 30s/epoch - 562ms/step\n",
      "Epoch 7/100\n",
      "53/53 - 30s - loss: 0.0420 - val_loss: 0.0432 - 30s/epoch - 564ms/step\n",
      "Epoch 8/100\n",
      "53/53 - 30s - loss: 0.0419 - val_loss: 0.0433 - 30s/epoch - 564ms/step\n",
      "Epoch 9/100\n",
      "53/53 - 30s - loss: 0.0419 - val_loss: 0.0433 - 30s/epoch - 561ms/step\n",
      "Epoch 10/100\n",
      "53/53 - 30s - loss: 0.0419 - val_loss: 0.0433 - 30s/epoch - 562ms/step\n",
      "Epoch 11/100\n",
      "53/53 - 29s - loss: 0.0419 - val_loss: 0.0434 - 29s/epoch - 554ms/step\n",
      "Epoch 12/100\n",
      "53/53 - 28s - loss: 0.0418 - val_loss: 0.0432 - 28s/epoch - 526ms/step\n",
      "Epoch 13/100\n",
      "53/53 - 30s - loss: 0.0419 - val_loss: 0.0435 - 30s/epoch - 561ms/step\n",
      "Epoch 14/100\n",
      "53/53 - 30s - loss: 0.0417 - val_loss: 0.0434 - 30s/epoch - 560ms/step\n",
      "Epoch 18/100\n",
      "53/53 - 30s - loss: 0.0417 - val_loss: 0.0437 - 30s/epoch - 561ms/step\n",
      "Epoch 19/100\n",
      "53/53 - 30s - loss: 0.0417 - val_loss: 0.0434 - 30s/epoch - 559ms/step\n",
      "Epoch 20/100\n",
      "53/53 - 29s - loss: 0.0416 - val_loss: 0.0435 - 29s/epoch - 542ms/step\n",
      "Epoch 21/100\n",
      "53/53 - 29s - loss: 0.0416 - val_loss: 0.0436 - 29s/epoch - 540ms/step\n",
      "Epoch 22/100\n",
      "53/53 - 30s - loss: 0.0416 - val_loss: 0.0436 - 30s/epoch - 558ms/step\n",
      "Epoch 23/100\n",
      "53/53 - 30s - loss: 0.0416 - val_loss: 0.0435 - 30s/epoch - 561ms/step\n",
      "Epoch 24/100\n",
      "53/53 - 30s - loss: 0.0415 - val_loss: 0.0436 - 30s/epoch - 560ms/step\n",
      "Epoch 25/100\n",
      "53/53 - 30s - loss: 0.0415 - val_loss: 0.0437 - 30s/epoch - 560ms/step\n",
      "Epoch 26/100\n",
      "53/53 - 30s - loss: 0.0414 - val_loss: 0.0436 - 30s/epoch - 561ms/step\n",
      "Epoch 27/100\n",
      "53/53 - 30s - loss: 0.0414 - val_loss: 0.0439 - 30s/epoch - 563ms/step\n",
      "Epoch 28/100\n",
      "53/53 - 30s - loss: 0.0414 - val_loss: 0.0436 - 30s/epoch - 560ms/step\n",
      "Epoch 29/100\n",
      "53/53 - 28s - loss: 0.0414 - val_loss: 0.0437 - 28s/epoch - 535ms/step\n",
      "Epoch 30/100\n",
      "53/53 - 29s - loss: 0.0413 - val_loss: 0.0438 - 29s/epoch - 551ms/step\n",
      "Epoch 31/100\n",
      "53/53 - 30s - loss: 0.0413 - val_loss: 0.0441 - 30s/epoch - 559ms/step\n",
      "Epoch 32/100\n",
      "53/53 - 30s - loss: 0.0413 - val_loss: 0.0438 - 30s/epoch - 560ms/step\n",
      "Epoch 33/100\n",
      "53/53 - 30s - loss: 0.0412 - val_loss: 0.0439 - 30s/epoch - 562ms/step\n",
      "Epoch 34/100\n",
      "53/53 - 30s - loss: 0.0413 - val_loss: 0.0439 - 30s/epoch - 564ms/step\n",
      "Epoch 35/100\n",
      "53/53 - 30s - loss: 0.0413 - val_loss: 0.0440 - 30s/epoch - 563ms/step\n",
      "Epoch 36/100\n",
      "53/53 - 30s - loss: 0.0412 - val_loss: 0.0438 - 30s/epoch - 563ms/step\n",
      "Epoch 37/100\n",
      "53/53 - 30s - loss: 0.0412 - val_loss: 0.0441 - 30s/epoch - 559ms/step\n",
      "Epoch 38/100\n",
      "53/53 - 28s - loss: 0.0411 - val_loss: 0.0439 - 28s/epoch - 526ms/step\n",
      "Epoch 39/100\n",
      "53/53 - 29s - loss: 0.0411 - val_loss: 0.0440 - 29s/epoch - 556ms/step\n",
      "Epoch 40/100\n",
      "53/53 - 30s - loss: 0.0410 - val_loss: 0.0441 - 30s/epoch - 559ms/step\n",
      "Epoch 41/100\n",
      "53/53 - 30s - loss: 0.0411 - val_loss: 0.0440 - 30s/epoch - 562ms/step\n",
      "Epoch 42/100\n",
      "53/53 - 30s - loss: 0.0411 - val_loss: 0.0446 - 30s/epoch - 560ms/step\n",
      "Epoch 43/100\n",
      "53/53 - 30s - loss: 0.0410 - val_loss: 0.0443 - 30s/epoch - 558ms/step\n",
      "Epoch 44/100\n",
      "53/53 - 30s - loss: 0.0409 - val_loss: 0.0443 - 30s/epoch - 561ms/step\n",
      "Epoch 45/100\n",
      "53/53 - 30s - loss: 0.0408 - val_loss: 0.0447 - 30s/epoch - 563ms/step\n",
      "Epoch 46/100\n",
      "53/53 - 30s - loss: 0.0408 - val_loss: 0.0444 - 30s/epoch - 559ms/step\n",
      "Epoch 47/100\n",
      "53/53 - 28s - loss: 0.0408 - val_loss: 0.0445 - 28s/epoch - 521ms/step\n",
      "Epoch 48/100\n",
      "53/53 - 30s - loss: 0.0407 - val_loss: 0.0448 - 30s/epoch - 559ms/step\n",
      "Epoch 53/100\n",
      "53/53 - 30s - loss: 0.0406 - val_loss: 0.0447 - 30s/epoch - 562ms/step\n",
      "Epoch 54/100\n",
      "53/53 - 30s - loss: 0.0406 - val_loss: 0.0450 - 30s/epoch - 560ms/step\n",
      "Epoch 55/100\n",
      "53/53 - 30s - loss: 0.0406 - val_loss: 0.0451 - 30s/epoch - 561ms/step\n",
      "Epoch 56/100\n",
      "53/53 - 27s - loss: 0.0405 - val_loss: 0.0445 - 27s/epoch - 519ms/step\n",
      "Epoch 57/100\n",
      "53/53 - 30s - loss: 0.0405 - val_loss: 0.0452 - 30s/epoch - 561ms/step\n",
      "Epoch 58/100\n",
      "53/53 - 30s - loss: 0.0405 - val_loss: 0.0452 - 30s/epoch - 559ms/step\n",
      "Epoch 59/100\n",
      "53/53 - 30s - loss: 0.0404 - val_loss: 0.0448 - 30s/epoch - 562ms/step\n",
      "Epoch 60/100\n",
      "53/53 - 30s - loss: 0.0404 - val_loss: 0.0455 - 30s/epoch - 565ms/step\n",
      "Epoch 61/100\n",
      "53/53 - 30s - loss: 0.0404 - val_loss: 0.0453 - 30s/epoch - 563ms/step\n",
      "Epoch 62/100\n",
      "53/53 - 30s - loss: 0.0403 - val_loss: 0.0455 - 30s/epoch - 560ms/step\n",
      "Epoch 63/100\n",
      "53/53 - 30s - loss: 0.0403 - val_loss: 0.0452 - 30s/epoch - 561ms/step\n",
      "Epoch 64/100\n",
      "53/53 - 30s - loss: 0.0402 - val_loss: 0.0452 - 30s/epoch - 559ms/step\n",
      "Epoch 65/100\n",
      "53/53 - 27s - loss: 0.0403 - val_loss: 0.0456 - 27s/epoch - 519ms/step\n",
      "Epoch 66/100\n",
      "53/53 - 30s - loss: 0.0402 - val_loss: 0.0456 - 30s/epoch - 562ms/step\n",
      "Epoch 67/100\n",
      "53/53 - 30s - loss: 0.0401 - val_loss: 0.0458 - 30s/epoch - 561ms/step\n",
      "Epoch 68/100\n",
      "53/53 - 30s - loss: 0.0401 - val_loss: 0.0455 - 30s/epoch - 560ms/step\n",
      "Epoch 69/100\n",
      "53/53 - 30s - loss: 0.0401 - val_loss: 0.0457 - 30s/epoch - 560ms/step\n",
      "Epoch 70/100\n",
      "53/53 - 30s - loss: 0.0401 - val_loss: 0.0457 - 30s/epoch - 563ms/step\n",
      "Epoch 71/100\n",
      "53/53 - 28s - loss: 0.0399 - val_loss: 0.0463 - 28s/epoch - 520ms/step\n",
      "Epoch 75/100\n",
      "53/53 - 30s - loss: 0.0399 - val_loss: 0.0464 - 30s/epoch - 565ms/step\n",
      "Epoch 76/100\n",
      "53/53 - 30s - loss: 0.0398 - val_loss: 0.0458 - 30s/epoch - 560ms/step\n",
      "Epoch 77/100\n",
      "53/53 - 30s - loss: 0.0398 - val_loss: 0.0462 - 30s/epoch - 558ms/step\n",
      "Epoch 78/100\n",
      "53/53 - 30s - loss: 0.0399 - val_loss: 0.0461 - 30s/epoch - 558ms/step\n",
      "Epoch 79/100\n",
      "53/53 - 30s - loss: 0.0397 - val_loss: 0.0466 - 30s/epoch - 563ms/step\n",
      "Epoch 80/100\n",
      "53/53 - 30s - loss: 0.0398 - val_loss: 0.0459 - 30s/epoch - 560ms/step\n",
      "Epoch 81/100\n",
      "53/53 - 30s - loss: 0.0397 - val_loss: 0.0465 - 30s/epoch - 559ms/step\n",
      "Epoch 82/100\n",
      "53/53 - 30s - loss: 0.0396 - val_loss: 0.0469 - 30s/epoch - 558ms/step\n",
      "Epoch 83/100\n",
      "53/53 - 28s - loss: 0.0396 - val_loss: 0.0468 - 28s/epoch - 526ms/step\n",
      "Epoch 84/100\n",
      "53/53 - 30s - loss: 0.0396 - val_loss: 0.0469 - 30s/epoch - 561ms/step\n",
      "Epoch 85/100\n",
      "53/53 - 30s - loss: 0.0397 - val_loss: 0.0468 - 30s/epoch - 564ms/step\n",
      "Epoch 86/100\n",
      "53/53 - 30s - loss: 0.0396 - val_loss: 0.0471 - 30s/epoch - 566ms/step\n",
      "Epoch 87/100\n",
      "53/53 - 30s - loss: 0.0395 - val_loss: 0.0466 - 30s/epoch - 561ms/step\n",
      "Epoch 88/100\n",
      "53/53 - 30s - loss: 0.0395 - val_loss: 0.0476 - 30s/epoch - 561ms/step\n",
      "Epoch 89/100\n",
      "53/53 - 30s - loss: 0.0395 - val_loss: 0.0462 - 30s/epoch - 559ms/step\n",
      "Epoch 90/100\n",
      "53/53 - 30s - loss: 0.0394 - val_loss: 0.0477 - 30s/epoch - 563ms/step\n",
      "Epoch 91/100\n",
      "53/53 - 29s - loss: 0.0394 - val_loss: 0.0469 - 29s/epoch - 547ms/step\n",
      "Epoch 92/100\n",
      "53/53 - 28s - loss: 0.0393 - val_loss: 0.0473 - 28s/epoch - 535ms/step\n",
      "Epoch 93/100\n",
      "53/53 - 30s - loss: 0.0392 - val_loss: 0.0476 - 30s/epoch - 561ms/step\n",
      "Epoch 94/100\n",
      "53/53 - 30s - loss: 0.0392 - val_loss: 0.0476 - 30s/epoch - 568ms/step\n",
      "Epoch 95/100\n",
      "53/53 - 30s - loss: 0.0392 - val_loss: 0.0475 - 30s/epoch - 565ms/step\n",
      "Epoch 96/100\n",
      "53/53 - 30s - loss: 0.0392 - val_loss: 0.0482 - 30s/epoch - 561ms/step\n",
      "Epoch 97/100\n",
      "53/53 - 30s - loss: 0.0391 - val_loss: 0.0477 - 30s/epoch - 559ms/step\n",
      "Epoch 98/100\n",
      "53/53 - 30s - loss: 0.0391 - val_loss: 0.0474 - 30s/epoch - 565ms/step\n",
      "Epoch 99/100\n",
      "53/53 - 30s - loss: 0.0390 - val_loss: 0.0481 - 30s/epoch - 560ms/step\n",
      "Epoch 100/100\n",
      "53/53 - 29s - loss: 0.0392 - val_loss: 0.0495 - 29s/epoch - 538ms/step\n",
      "current_epoch:  200\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch\n",
    "print('current_epoch: ', current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:00<00:00,  2.17s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "53/53 - 30s - loss: 0.0391 - val_loss: 0.0487 - 30s/epoch - 559ms/step\n",
      "Epoch 2/200\n",
      "53/53 - 30s - loss: 0.0390 - val_loss: 0.0492 - 30s/epoch - 562ms/step\n",
      "Epoch 6/200\n",
      "53/53 - 29s - loss: 0.0389 - val_loss: 0.0487 - 29s/epoch - 538ms/step\n",
      "Epoch 7/200\n",
      "53/53 - 29s - loss: 0.0390 - val_loss: 0.0486 - 29s/epoch - 542ms/step\n",
      "Epoch 8/200\n",
      "53/53 - 30s - loss: 0.0388 - val_loss: 0.0482 - 30s/epoch - 561ms/step\n",
      "Epoch 9/200\n",
      "53/53 - 30s - loss: 0.0388 - val_loss: 0.0496 - 30s/epoch - 562ms/step\n",
      "Epoch 10/200\n",
      "53/53 - 30s - loss: 0.0388 - val_loss: 0.0485 - 30s/epoch - 563ms/step\n",
      "Epoch 11/200\n",
      "53/53 - 30s - loss: 0.0387 - val_loss: 0.0481 - 30s/epoch - 562ms/step\n",
      "Epoch 12/200\n",
      "53/53 - 30s - loss: 0.0387 - val_loss: 0.0488 - 30s/epoch - 563ms/step\n",
      "Epoch 13/200\n",
      "53/53 - 30s - loss: 0.0386 - val_loss: 0.0484 - 30s/epoch - 561ms/step\n",
      "Epoch 14/200\n",
      "53/53 - 30s - loss: 0.0386 - val_loss: 0.0494 - 30s/epoch - 560ms/step\n",
      "Epoch 15/200\n",
      "53/53 - 28s - loss: 0.0385 - val_loss: 0.0486 - 28s/epoch - 531ms/step\n",
      "Epoch 16/200\n",
      "53/53 - 29s - loss: 0.0386 - val_loss: 0.0487 - 29s/epoch - 555ms/step\n",
      "Epoch 17/200\n",
      "53/53 - 30s - loss: 0.0385 - val_loss: 0.0494 - 30s/epoch - 562ms/step\n",
      "Epoch 18/200\n",
      "53/53 - 30s - loss: 0.0386 - val_loss: 0.0493 - 30s/epoch - 568ms/step\n",
      "Epoch 19/200\n",
      "53/53 - 30s - loss: 0.0385 - val_loss: 0.0496 - 30s/epoch - 562ms/step\n",
      "Epoch 20/200\n",
      "53/53 - 30s - loss: 0.0385 - val_loss: 0.0505 - 30s/epoch - 561ms/step\n",
      "Epoch 21/200\n",
      "53/53 - 30s - loss: 0.0385 - val_loss: 0.0487 - 30s/epoch - 561ms/step\n",
      "Epoch 22/200\n",
      "53/53 - 30s - loss: 0.0384 - val_loss: 0.0489 - 30s/epoch - 559ms/step\n",
      "Epoch 23/200\n",
      "53/53 - 30s - loss: 0.0385 - val_loss: 0.0493 - 30s/epoch - 559ms/step\n",
      "Epoch 24/200\n",
      "53/53 - 28s - loss: 0.0385 - val_loss: 0.0496 - 28s/epoch - 520ms/step\n",
      "Epoch 25/200\n",
      "53/53 - 30s - loss: 0.0383 - val_loss: 0.0507 - 30s/epoch - 566ms/step\n",
      "Epoch 26/200\n",
      "53/53 - 30s - loss: 0.0383 - val_loss: 0.0502 - 30s/epoch - 564ms/step\n",
      "Epoch 27/200\n",
      "53/53 - 30s - loss: 0.0382 - val_loss: 0.0503 - 30s/epoch - 560ms/step\n",
      "Epoch 28/200\n",
      "53/53 - 30s - loss: 0.0382 - val_loss: 0.0500 - 30s/epoch - 562ms/step\n",
      "Epoch 29/200\n",
      "53/53 - 30s - loss: 0.0384 - val_loss: 0.0503 - 30s/epoch - 558ms/step\n",
      "Epoch 30/200\n",
      "53/53 - 30s - loss: 0.0381 - val_loss: 0.0514 - 30s/epoch - 559ms/step\n",
      "Epoch 31/200\n",
      "53/53 - 30s - loss: 0.0381 - val_loss: 0.0508 - 30s/epoch - 559ms/step\n",
      "Epoch 32/200\n",
      "53/53 - 30s - loss: 0.0381 - val_loss: 0.0497 - 30s/epoch - 559ms/step\n",
      "Epoch 33/200\n",
      "53/53 - 28s - loss: 0.0381 - val_loss: 0.0506 - 28s/epoch - 522ms/step\n",
      "Epoch 34/200\n",
      "53/53 - 30s - loss: 0.0380 - val_loss: 0.0510 - 30s/epoch - 559ms/step\n",
      "Epoch 35/200\n",
      "53/53 - 30s - loss: 0.0379 - val_loss: 0.0513 - 30s/epoch - 561ms/step\n",
      "Epoch 36/200\n",
      "53/53 - 30s - loss: 0.0379 - val_loss: 0.0506 - 30s/epoch - 561ms/step\n",
      "Epoch 37/200\n",
      "53/53 - 30s - loss: 0.0378 - val_loss: 0.0507 - 30s/epoch - 558ms/step\n",
      "Epoch 41/200\n",
      "53/53 - 30s - loss: 0.0378 - val_loss: 0.0516 - 30s/epoch - 559ms/step\n",
      "Epoch 42/200\n",
      "53/53 - 28s - loss: 0.0379 - val_loss: 0.0500 - 28s/epoch - 523ms/step\n",
      "Epoch 43/200\n",
      "53/53 - 30s - loss: 0.0378 - val_loss: 0.0514 - 30s/epoch - 558ms/step\n",
      "Epoch 44/200\n",
      "53/53 - 30s - loss: 0.0379 - val_loss: 0.0519 - 30s/epoch - 560ms/step\n",
      "Epoch 45/200\n",
      "53/53 - 30s - loss: 0.0378 - val_loss: 0.0504 - 30s/epoch - 561ms/step\n",
      "Epoch 46/200\n",
      "53/53 - 30s - loss: 0.0379 - val_loss: 0.0512 - 30s/epoch - 558ms/step\n",
      "Epoch 47/200\n",
      "53/53 - 30s - loss: 0.0377 - val_loss: 0.0516 - 30s/epoch - 564ms/step\n",
      "Epoch 48/200\n",
      "53/53 - 30s - loss: 0.0377 - val_loss: 0.0509 - 30s/epoch - 558ms/step\n",
      "Epoch 49/200\n",
      "53/53 - 30s - loss: 0.0377 - val_loss: 0.0525 - 30s/epoch - 561ms/step\n",
      "Epoch 50/200\n",
      "53/53 - 30s - loss: 0.0376 - val_loss: 0.0524 - 30s/epoch - 563ms/step\n",
      "Epoch 51/200\n",
      "53/53 - 28s - loss: 0.0376 - val_loss: 0.0535 - 28s/epoch - 523ms/step\n",
      "Epoch 52/200\n",
      "53/53 - 30s - loss: 0.0376 - val_loss: 0.0525 - 30s/epoch - 561ms/step\n",
      "Epoch 53/200\n",
      "53/53 - 30s - loss: 0.0376 - val_loss: 0.0516 - 30s/epoch - 567ms/step\n",
      "Epoch 54/200\n",
      "53/53 - 30s - loss: 0.0375 - val_loss: 0.0517 - 30s/epoch - 560ms/step\n",
      "Epoch 55/200\n",
      "53/53 - 30s - loss: 0.0375 - val_loss: 0.0526 - 30s/epoch - 563ms/step\n",
      "Epoch 56/200\n",
      "53/53 - 30s - loss: 0.0376 - val_loss: 0.0524 - 30s/epoch - 561ms/step\n",
      "Epoch 57/200\n",
      "53/53 - 30s - loss: 0.0374 - val_loss: 0.0529 - 30s/epoch - 561ms/step\n",
      "Epoch 58/200\n",
      "53/53 - 30s - loss: 0.0375 - val_loss: 0.0544 - 30s/epoch - 562ms/step\n",
      "Epoch 59/200\n",
      "53/53 - 30s - loss: 0.0375 - val_loss: 0.0533 - 30s/epoch - 564ms/step\n",
      "Epoch 60/200\n",
      "53/53 - 28s - loss: 0.0375 - val_loss: 0.0530 - 28s/epoch - 522ms/step\n",
      "Epoch 61/200\n",
      "53/53 - 30s - loss: 0.0373 - val_loss: 0.0530 - 30s/epoch - 561ms/step\n",
      "Epoch 62/200\n",
      "53/53 - 30s - loss: 0.0373 - val_loss: 0.0533 - 30s/epoch - 560ms/step\n",
      "Epoch 63/200\n",
      "53/53 - 30s - loss: 0.0372 - val_loss: 0.0532 - 30s/epoch - 562ms/step\n",
      "Epoch 64/200\n",
      "53/53 - 30s - loss: 0.0373 - val_loss: 0.0525 - 30s/epoch - 561ms/step\n",
      "Epoch 65/200\n",
      "53/53 - 30s - loss: 0.0372 - val_loss: 0.0538 - 30s/epoch - 559ms/step\n",
      "Epoch 66/200\n",
      "53/53 - 30s - loss: 0.0372 - val_loss: 0.0525 - 30s/epoch - 559ms/step\n",
      "Epoch 67/200\n",
      "53/53 - 30s - loss: 0.0373 - val_loss: 0.0537 - 30s/epoch - 561ms/step\n",
      "Epoch 68/200\n",
      "53/53 - 29s - loss: 0.0373 - val_loss: 0.0543 - 29s/epoch - 548ms/step\n",
      "Epoch 69/200\n",
      "53/53 - 28s - loss: 0.0374 - val_loss: 0.0544 - 28s/epoch - 532ms/step\n",
      "Epoch 70/200\n",
      "53/53 - 30s - loss: 0.0371 - val_loss: 0.0561 - 30s/epoch - 560ms/step\n",
      "Epoch 71/200\n",
      "53/53 - 30s - loss: 0.0370 - val_loss: 0.0535 - 30s/epoch - 563ms/step\n",
      "Epoch 72/200\n",
      "53/53 - 30s - loss: 0.0370 - val_loss: 0.0542 - 30s/epoch - 564ms/step\n",
      "Epoch 76/200\n",
      "53/53 - 30s - loss: 0.0369 - val_loss: 0.0542 - 30s/epoch - 562ms/step\n",
      "Epoch 77/200\n",
      "53/53 - 29s - loss: 0.0369 - val_loss: 0.0546 - 29s/epoch - 540ms/step\n",
      "Epoch 78/200\n",
      "53/53 - 29s - loss: 0.0370 - val_loss: 0.0552 - 29s/epoch - 543ms/step\n",
      "Epoch 79/200\n",
      "53/53 - 30s - loss: 0.0370 - val_loss: 0.0520 - 30s/epoch - 559ms/step\n",
      "Epoch 80/200\n",
      "53/53 - 30s - loss: 0.0373 - val_loss: 0.0538 - 30s/epoch - 560ms/step\n",
      "Epoch 81/200\n",
      "53/53 - 30s - loss: 0.0371 - val_loss: 0.0549 - 30s/epoch - 559ms/step\n",
      "Epoch 82/200\n",
      "53/53 - 30s - loss: 0.0369 - val_loss: 0.0554 - 30s/epoch - 563ms/step\n",
      "Epoch 83/200\n",
      "53/53 - 30s - loss: 0.0368 - val_loss: 0.0564 - 30s/epoch - 559ms/step\n",
      "Epoch 84/200\n",
      "53/53 - 30s - loss: 0.0367 - val_loss: 0.0536 - 30s/epoch - 560ms/step\n",
      "Epoch 85/200\n",
      "53/53 - 30s - loss: 0.0367 - val_loss: 0.0544 - 30s/epoch - 564ms/step\n",
      "Epoch 86/200\n",
      "53/53 - 28s - loss: 0.0368 - val_loss: 0.0565 - 28s/epoch - 532ms/step\n",
      "Epoch 87/200\n",
      "53/53 - 29s - loss: 0.0369 - val_loss: 0.0556 - 29s/epoch - 551ms/step\n",
      "Epoch 88/200\n",
      "53/53 - 30s - loss: 0.0368 - val_loss: 0.0551 - 30s/epoch - 568ms/step\n",
      "Epoch 89/200\n",
      "53/53 - 30s - loss: 0.0367 - val_loss: 0.0578 - 30s/epoch - 561ms/step\n",
      "Epoch 90/200\n",
      "53/53 - 30s - loss: 0.0367 - val_loss: 0.0550 - 30s/epoch - 561ms/step\n",
      "Epoch 91/200\n",
      "53/53 - 30s - loss: 0.0367 - val_loss: 0.0552 - 30s/epoch - 561ms/step\n",
      "Epoch 92/200\n",
      "53/53 - 30s - loss: 0.0366 - val_loss: 0.0546 - 30s/epoch - 561ms/step\n",
      "Epoch 93/200\n",
      "53/53 - 30s - loss: 0.0366 - val_loss: 0.0538 - 30s/epoch - 562ms/step\n",
      "Epoch 94/200\n",
      "53/53 - 30s - loss: 0.0368 - val_loss: 0.0551 - 30s/epoch - 566ms/step\n",
      "Epoch 95/200\n",
      "53/53 - 28s - loss: 0.0365 - val_loss: 0.0564 - 28s/epoch - 519ms/step\n",
      "Epoch 96/200\n",
      "53/53 - 30s - loss: 0.0366 - val_loss: 0.0548 - 30s/epoch - 560ms/step\n",
      "Epoch 97/200\n",
      "53/53 - 30s - loss: 0.0366 - val_loss: 0.0543 - 30s/epoch - 562ms/step\n",
      "Epoch 98/200\n",
      "53/53 - 30s - loss: 0.0364 - val_loss: 0.0552 - 30s/epoch - 560ms/step\n",
      "Epoch 99/200\n",
      "53/53 - 30s - loss: 0.0364 - val_loss: 0.0567 - 30s/epoch - 564ms/step\n",
      "Epoch 100/200\n",
      "53/53 - 30s - loss: 0.0364 - val_loss: 0.0542 - 30s/epoch - 563ms/step\n",
      "Epoch 101/200\n",
      "53/53 - 30s - loss: 0.0365 - val_loss: 0.0558 - 30s/epoch - 562ms/step\n",
      "Epoch 102/200\n",
      "53/53 - 30s - loss: 0.0365 - val_loss: 0.0569 - 30s/epoch - 563ms/step\n",
      "Epoch 103/200\n",
      "53/53 - 30s - loss: 0.0365 - val_loss: 0.0557 - 30s/epoch - 562ms/step\n",
      "Epoch 104/200\n",
      "53/53 - 28s - loss: 0.0365 - val_loss: 0.0559 - 28s/epoch - 525ms/step\n",
      "Epoch 105/200\n",
      "53/53 - 30s - loss: 0.0363 - val_loss: 0.0551 - 30s/epoch - 560ms/step\n",
      "Epoch 109/200\n",
      "53/53 - 30s - loss: 0.0363 - val_loss: 0.0552 - 30s/epoch - 559ms/step\n",
      "Epoch 110/200\n",
      "53/53 - 30s - loss: 0.0363 - val_loss: 0.0590 - 30s/epoch - 560ms/step\n",
      "Epoch 111/200\n",
      "53/53 - 30s - loss: 0.0363 - val_loss: 0.0565 - 30s/epoch - 560ms/step\n",
      "Epoch 112/200\n",
      "53/53 - 30s - loss: 0.0363 - val_loss: 0.0564 - 30s/epoch - 564ms/step\n",
      "Epoch 113/200\n",
      "53/53 - 28s - loss: 0.0362 - val_loss: 0.0574 - 28s/epoch - 523ms/step\n",
      "Epoch 114/200\n",
      "53/53 - 30s - loss: 0.0363 - val_loss: 0.0596 - 30s/epoch - 562ms/step\n",
      "Epoch 115/200\n",
      "53/53 - 30s - loss: 0.0361 - val_loss: 0.0569 - 30s/epoch - 561ms/step\n",
      "Epoch 116/200\n",
      "53/53 - 30s - loss: 0.0362 - val_loss: 0.0574 - 30s/epoch - 564ms/step\n",
      "Epoch 117/200\n",
      "53/53 - 30s - loss: 0.0364 - val_loss: 0.0566 - 30s/epoch - 560ms/step\n",
      "Epoch 118/200\n",
      "53/53 - 30s - loss: 0.0361 - val_loss: 0.0562 - 30s/epoch - 560ms/step\n",
      "Epoch 119/200\n",
      "53/53 - 30s - loss: 0.0361 - val_loss: 0.0569 - 30s/epoch - 559ms/step\n",
      "Epoch 120/200\n",
      "53/53 - 30s - loss: 0.0361 - val_loss: 0.0566 - 30s/epoch - 561ms/step\n",
      "Epoch 121/200\n",
      "53/53 - 30s - loss: 0.0362 - val_loss: 0.0592 - 30s/epoch - 563ms/step\n",
      "Epoch 122/200\n",
      "53/53 - 28s - loss: 0.0363 - val_loss: 0.0581 - 28s/epoch - 522ms/step\n",
      "Epoch 123/200\n",
      "53/53 - 30s - loss: 0.0361 - val_loss: 0.0564 - 30s/epoch - 557ms/step\n",
      "Epoch 124/200\n",
      "53/53 - 30s - loss: 0.0361 - val_loss: 0.0576 - 30s/epoch - 561ms/step\n",
      "Epoch 125/200\n",
      "53/53 - 30s - loss: 0.0360 - val_loss: 0.0583 - 30s/epoch - 564ms/step\n",
      "Epoch 126/200\n",
      "53/53 - 30s - loss: 0.0363 - val_loss: 0.0566 - 30s/epoch - 562ms/step\n",
      "Epoch 127/200\n",
      "53/53 - 30s - loss: 0.0360 - val_loss: 0.0573 - 30s/epoch - 559ms/step\n",
      "Epoch 128/200\n",
      "53/53 - 30s - loss: 0.0361 - val_loss: 0.0569 - 30s/epoch - 560ms/step\n",
      "Epoch 129/200\n",
      "53/53 - 30s - loss: 0.0360 - val_loss: 0.0585 - 30s/epoch - 562ms/step\n",
      "Epoch 130/200\n",
      "53/53 - 29s - loss: 0.0360 - val_loss: 0.0585 - 29s/epoch - 556ms/step\n",
      "Epoch 131/200\n",
      "53/53 - 28s - loss: 0.0358 - val_loss: 0.0574 - 28s/epoch - 529ms/step\n",
      "Epoch 132/200\n",
      "53/53 - 30s - loss: 0.0359 - val_loss: 0.0589 - 30s/epoch - 559ms/step\n",
      "Epoch 133/200\n",
      "53/53 - 30s - loss: 0.0360 - val_loss: 0.0575 - 30s/epoch - 561ms/step\n",
      "Epoch 134/200\n",
      "53/53 - 30s - loss: 0.0359 - val_loss: 0.0570 - 30s/epoch - 563ms/step\n",
      "Epoch 135/200\n",
      "53/53 - 30s - loss: 0.0358 - val_loss: 0.0586 - 30s/epoch - 560ms/step\n",
      "Epoch 136/200\n",
      "53/53 - 30s - loss: 0.0358 - val_loss: 0.0582 - 30s/epoch - 564ms/step\n",
      "Epoch 137/200\n",
      "53/53 - 30s - loss: 0.0357 - val_loss: 0.0587 - 30s/epoch - 560ms/step\n",
      "Epoch 138/200\n",
      "53/53 - 30s - loss: 0.0358 - val_loss: 0.0614 - 30s/epoch - 560ms/step\n",
      "Epoch 139/200\n",
      "53/53 - 30s - loss: 0.0356 - val_loss: 0.0601 - 30s/epoch - 561ms/step\n",
      "Epoch 143/200\n",
      "53/53 - 30s - loss: 0.0356 - val_loss: 0.0597 - 30s/epoch - 562ms/step\n",
      "Epoch 144/200\n",
      "53/53 - 30s - loss: 0.0358 - val_loss: 0.0584 - 30s/epoch - 559ms/step\n",
      "Epoch 145/200\n",
      "53/53 - 30s - loss: 0.0358 - val_loss: 0.0603 - 30s/epoch - 568ms/step\n",
      "Epoch 146/200\n",
      "53/53 - 30s - loss: 0.0356 - val_loss: 0.0585 - 30s/epoch - 560ms/step\n",
      "Epoch 147/200\n",
      "53/53 - 30s - loss: 0.0356 - val_loss: 0.0593 - 30s/epoch - 561ms/step\n",
      "Epoch 148/200\n",
      "53/53 - 28s - loss: 0.0357 - val_loss: 0.0609 - 28s/epoch - 535ms/step\n",
      "Epoch 149/200\n",
      "53/53 - 29s - loss: 0.0356 - val_loss: 0.0616 - 29s/epoch - 544ms/step\n",
      "Epoch 150/200\n",
      "53/53 - 30s - loss: 0.0357 - val_loss: 0.0616 - 30s/epoch - 561ms/step\n",
      "Epoch 151/200\n",
      "53/53 - 30s - loss: 0.0356 - val_loss: 0.0608 - 30s/epoch - 561ms/step\n",
      "Epoch 152/200\n",
      "53/53 - 30s - loss: 0.0355 - val_loss: 0.0604 - 30s/epoch - 562ms/step\n",
      "Epoch 153/200\n",
      "53/53 - 30s - loss: 0.0355 - val_loss: 0.0608 - 30s/epoch - 565ms/step\n",
      "Epoch 154/200\n",
      "53/53 - 30s - loss: 0.0355 - val_loss: 0.0623 - 30s/epoch - 562ms/step\n",
      "Epoch 155/200\n",
      "53/53 - 30s - loss: 0.0355 - val_loss: 0.0634 - 30s/epoch - 566ms/step\n",
      "Epoch 156/200\n",
      "53/53 - 30s - loss: 0.0355 - val_loss: 0.0601 - 30s/epoch - 560ms/step\n",
      "Epoch 157/200\n",
      "53/53 - 28s - loss: 0.0355 - val_loss: 0.0638 - 28s/epoch - 534ms/step\n",
      "Epoch 158/200\n",
      "53/53 - 30s - loss: 0.0357 - val_loss: 0.0581 - 30s/epoch - 557ms/step\n",
      "Epoch 159/200\n",
      "53/53 - 30s - loss: 0.0356 - val_loss: 0.0588 - 30s/epoch - 559ms/step\n",
      "Epoch 160/200\n",
      "53/53 - 30s - loss: 0.0355 - val_loss: 0.0619 - 30s/epoch - 560ms/step\n",
      "Epoch 161/200\n",
      "53/53 - 30s - loss: 0.0355 - val_loss: 0.0615 - 30s/epoch - 562ms/step\n",
      "Epoch 162/200\n",
      "53/53 - 30s - loss: 0.0354 - val_loss: 0.0614 - 30s/epoch - 562ms/step\n",
      "Epoch 163/200\n",
      "53/53 - 30s - loss: 0.0354 - val_loss: 0.0603 - 30s/epoch - 565ms/step\n",
      "Epoch 164/200\n",
      "53/53 - 30s - loss: 0.0354 - val_loss: 0.0595 - 30s/epoch - 563ms/step\n",
      "Epoch 165/200\n",
      "53/53 - 30s - loss: 0.0354 - val_loss: 0.0626 - 30s/epoch - 566ms/step\n",
      "Epoch 166/200\n",
      "53/53 - 28s - loss: 0.0353 - val_loss: 0.0625 - 28s/epoch - 522ms/step\n",
      "Epoch 167/200\n",
      "53/53 - 30s - loss: 0.0353 - val_loss: 0.0613 - 30s/epoch - 557ms/step\n",
      "Epoch 168/200\n",
      "53/53 - 30s - loss: 0.0354 - val_loss: 0.0621 - 30s/epoch - 559ms/step\n",
      "Epoch 169/200\n",
      "53/53 - 30s - loss: 0.0357 - val_loss: 0.0612 - 30s/epoch - 562ms/step\n",
      "Epoch 170/200\n",
      "53/53 - 30s - loss: 0.0355 - val_loss: 0.0621 - 30s/epoch - 562ms/step\n",
      "Epoch 171/200\n",
      "53/53 - 30s - loss: 0.0352 - val_loss: 0.0624 - 30s/epoch - 563ms/step\n",
      "Epoch 172/200\n",
      "53/53 - 30s - loss: 0.0353 - val_loss: 0.0606 - 30s/epoch - 563ms/step\n",
      "Epoch 177/200\n",
      "53/53 - 30s - loss: 0.0352 - val_loss: 0.0631 - 30s/epoch - 561ms/step\n",
      "Epoch 178/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0635 - 30s/epoch - 565ms/step\n",
      "Epoch 179/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0608 - 30s/epoch - 561ms/step\n",
      "Epoch 180/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0617 - 30s/epoch - 560ms/step\n",
      "Epoch 181/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0627 - 30s/epoch - 562ms/step\n",
      "Epoch 182/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0624 - 30s/epoch - 558ms/step\n",
      "Epoch 183/200\n",
      "53/53 - 30s - loss: 0.0350 - val_loss: 0.0613 - 30s/epoch - 558ms/step\n",
      "Epoch 184/200\n",
      "53/53 - 28s - loss: 0.0351 - val_loss: 0.0612 - 28s/epoch - 520ms/step\n",
      "Epoch 185/200\n",
      "53/53 - 30s - loss: 0.0352 - val_loss: 0.0625 - 30s/epoch - 560ms/step\n",
      "Epoch 186/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0624 - 30s/epoch - 559ms/step\n",
      "Epoch 187/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0603 - 30s/epoch - 565ms/step\n",
      "Epoch 188/200\n",
      "53/53 - 30s - loss: 0.0350 - val_loss: 0.0635 - 30s/epoch - 559ms/step\n",
      "Epoch 189/200\n",
      "53/53 - 30s - loss: 0.0349 - val_loss: 0.0636 - 30s/epoch - 559ms/step\n",
      "Epoch 190/200\n",
      "53/53 - 30s - loss: 0.0350 - val_loss: 0.0626 - 30s/epoch - 562ms/step\n",
      "Epoch 191/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0639 - 30s/epoch - 560ms/step\n",
      "Epoch 192/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0607 - 30s/epoch - 561ms/step\n",
      "Epoch 193/200\n",
      "53/53 - 28s - loss: 0.0351 - val_loss: 0.0607 - 28s/epoch - 524ms/step\n",
      "Epoch 194/200\n",
      "53/53 - 30s - loss: 0.0350 - val_loss: 0.0600 - 30s/epoch - 562ms/step\n",
      "Epoch 195/200\n",
      "53/53 - 30s - loss: 0.0350 - val_loss: 0.0647 - 30s/epoch - 559ms/step\n",
      "Epoch 196/200\n",
      "53/53 - 30s - loss: 0.0350 - val_loss: 0.0631 - 30s/epoch - 560ms/step\n",
      "Epoch 197/200\n",
      "53/53 - 30s - loss: 0.0351 - val_loss: 0.0620 - 30s/epoch - 559ms/step\n",
      "Epoch 198/200\n",
      "53/53 - 30s - loss: 0.0349 - val_loss: 0.0614 - 30s/epoch - 557ms/step\n",
      "Epoch 199/200\n",
      "53/53 - 30s - loss: 0.0349 - val_loss: 0.0650 - 30s/epoch - 566ms/step\n",
      "Epoch 200/200\n",
      "53/53 - 30s - loss: 0.0350 - val_loss: 0.0634 - 30s/epoch - 558ms/step\n",
      "current_epoch:  400\n"
     ]
    }
   ],
   "source": [
    "epoch = 200\n",
    "pixel_cnn.fit(\n",
    "    data_train, data_train, validation_data=(data_test, data_test), batch_size=128, epochs=epoch, verbose=2\n",
    ")\n",
    "current_epoch += epoch\n",
    "print('current_epoch: ', current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:59<00:00,  2.12s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty array of pixels.\n",
    "batch = 10\n",
    "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in tqdm(range(rows)):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "            # pixel.\n",
    "            probs = pixel_cnn.predict(pixels, verbose=0)[:, row, col, channel]\n",
    "            # Use the probabilities to pick pixel values and append the values to the image\n",
    "            # frame.\n",
    "            pixels[:, row, col, channel] = tf.math.ceil(\n",
    "                probs - tf.random.uniform(probs.shape)\n",
    "            )\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Stack the single channeled black and white image to RGB values.\n",
    "    x = np.stack((x, x, x), 2)\n",
    "    # Undo preprocessing\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    keras.utils.save_img(\n",
    "        f\"{d}-generated_image-ep_{current_epoch}-{i}.png\", deprocess_image(np.squeeze(pic, -1))\n",
    "    )\n",
    "    \n",
    "# 생성된 샘플 display\n",
    "plt.figure(figsize=(20,2))\n",
    "for c in range(10):\n",
    "    plt.subplot(1,10,c+1)\n",
    "    plt.imshow(plt.imread(f\"{d}-generated_image-ep_{current_epoch}-{c}.png\"),cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
