# 2023.09.11

### 참여자

학부 : 김찬호, 김은지, 문채원, 김건아

멘토 : 박상효 교수님, 연구원 2명

### 회의내용

- 수행계획서: 이번주까지 작성하기
- 결과보고서: 12월 말까지 작성하면 됨
- 논문 발표 일정: 추계 or 동계

1. 주제설명
    
    ***메타버스용 배경 합성 + Retargeting***
    
    실제로 카메라로 사진/동영상 찍고, 사람 얼굴이나 몸 캡처해서 아바타로 두고
    백그라운드만 메타버스에 맞게 다양한 배경에서 쓸 수 있도록 하는 것이 목표
    object와 background 구분해서 작동시킬 것임
    
    동영상을 카메라로 찍으면 360도 전체를 찍을 수 없음.
    → 카메라 여러대 찍어야함. (자신의 뒷모습 포함한 아바타 만들려면)
    
    문제점: 카메라의 위치 달라짐+아바타에 그림자 생김, 햇빛 제외가 어려움, 어두우면 또 못찍음
    ⇒ 야외 메타버스 찍는 것은 아직 불가능함. (퀄리티 떨어짐)
    
    객체를 분리한 다음에 만든 배경을 메타버스 화면에 진짜처럼 보여야 하는데
    배경을 사람 눈으로 보는 것 만큼의 퀄리티를 내려면 연결선이 두꺼워져야함.
    뒤의 모습을 볼 필요가 없지 않을까?
    ⇒ resizing ( 사람 시야각에 맞게 자연스럽게, 옆에도 살짝 보이게 )
    
    - 화질 개선하면서 super resolution(해상도 변환)
    - inpainting-그럴듯하게 칠한다 (자연스럽게 채워줌 )
    ⇒ 이 과정에서 GAN을 사용할 것이다.
    
2. 진행과정
    1. 데이터셋 정리
    2. 웹에 있는 메타버스 컨텐츠/데이터 정리
    3. 딥러닝 코드를 돌려보는 것 (semantic segmentation)
    → 객체와 배경 분리 + 배경 다시 칠하기
    → 객체 마지막에 다시 붙이기( 원본 화면만 가로로 늘리는게 아니라 객체는 살리고 배경만 늘리면 어색하지 않을 것이다! )
    
3. 현재까지의 연구
    
    Sementic Segmentation inpainting
    - 딥러닝 코드를 새로운 데이터셋(메타버스 컨텐츠)으로 돌려보기, resizing
    ⇒ 이 정도는 이미 완료됨
    
    문제점 : 생성된 비디오를 사용하면 성능이 떨어진다!
    → 자연 데이터로만 학습하니까 생성 비디오를 사용하니 문제점이 발생한다는 것, 어설프게 표현된다.
    
    이러한 문제점을 메타버스에서 파악만 하고 끝내도 됨. 아니면 더 나아가서 이런 에러를 줄이기 위해 생성 영상(메타버스 영상)에 최적화된 기법을 만들면 더 좋다!
    (딥러닝 코드로 데이터들 쭉 돌려서 나온 결과 정리 해서 11월 초나 말에 동계 학술대회에 내도 되는지..?  빠르게는 10월 말에 논문 작업을 할 예정 )
    
    - 메타버스 데이터 수집하고, 딥러닝 했을 때 결과 영상 보면서 어디가 잘 되고 어디가 잘 안되는지 찾기.
    - 거기서 더 나아가서 다른 딥러닝 모델 가져와서 ‘GAN 모델 써서 화질 개선하거나 어색함을 사라지게’ 개선하는 작업까지 하면 더 좋다. 메타버스에 최적화 된 기법을 만들면 더 좋음.
    
4. 요약
    1. 데이터셋 정리하기 ( 어색한 점 찾기 )
    2. GAN 모델까지 돌려서 어색한 점 개선이 되는지 파악
    3. 논문 참여하는 사람은 point cloud data에 코드 적용해볼 것이다.( 그래픽스 관련 내용)
    → 논문 참여 하게 되면 글쓰는 작업(GAN 적용하는 과정 매뉴얼 등),
    기존 논문 리뷰, 논문 약간 작성함. 등등 논문은 보통 2~4P
    4. 개발환경은 코랩을 이용할 예정